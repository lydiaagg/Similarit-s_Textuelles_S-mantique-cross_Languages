{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Neural_Network _(DNN).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyCAqBpNCnPq"
      },
      "source": [
        "%%capture\n",
        "# Install the latest Tensorflow version.\n",
        "!pip install tensorflow_text\n",
        "!pip install bokeh\n",
        "!pip install simpleneighbors[annoy]\n",
        "!pip install tqdm\n",
        "!pip install unidecode"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JlNt2Iu72or"
      },
      "source": [
        "import os,re,collections\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GRU,LSTM, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "import tensorflow.compat.v2 as tf\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T82KUFf0s2u",
        "outputId": "18bedd9d-4cf7-4eb2-8659-cce6eb02c034"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vt2rQCOPz74"
      },
      "source": [
        "## ***Etape 1 : Chargement de données*** \n",
        " Nous allons extraire les données qui se trouvent dans le zip dans le drive ainsi pouvoirs les exploité facilement puis nous allons organiser les fichier de sort a ce que chaque phrase aille sa traduction dans le fichier correspondant,les phrase qui seront extraitent du fichier vont étre prétraiter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbD5Gp2uf6Fo"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "#nous allons extraire nos données qui se trouvent dans un fichier zip dans notre drive \n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/rebuilt.Cross-Language-Dataset-master.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NohB7jO3jL0w"
      },
      "source": [
        "#nous allons experimenter une partie de nos donnée dans cette partie on commence par le corpus APR \n",
        "English_Dataset = tf.data.Dataset.list_files(\"/tmp/Cross-Language-Dataset-master/dataset/chunks/APR/en/*.txt\")\n",
        "Frensh_Dataset = tf.data.Dataset.list_files(\"/tmp/Cross-Language-Dataset-master/dataset/chunks/APR/fr/*.txt\")\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH58fu-j_in7"
      },
      "source": [
        "#nous allons organiser nos fichier text de sort a ce que les fichier en francais aille leur traduction anglaise\n",
        "English_DatasetFiles=list(English_Dataset.as_numpy_iterator())\n",
        "English_DatasetFiles.sort()\n",
        "Frensh_DatasetFiles=list(Frensh_Dataset .as_numpy_iterator())\n",
        "Frensh_DatasetFiles.sort()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtJR_FFUUQ0d"
      },
      "source": [
        "#cette fonction permet d'extraire extrait les donnees des fichier\n",
        "def load_data(Files):\n",
        "  sentences=[]\n",
        "  for file_content in Files: \n",
        "    with open(file_content, \"r\") as f:\n",
        "      data = f.read()\n",
        "      sentences.append(data)\n",
        "  return sentences\n",
        "\n",
        "english_sentences=load_data(English_DatasetFiles)\n",
        "french_sentences=load_data(Frensh_DatasetFiles)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITcf0jAdfy6u",
        "outputId": "cc1c3672-25a4-4183-d344-6ac72433f6aa"
      },
      "source": [
        "for i in range(5):\n",
        "  print('Sample :',i)\n",
        "  print(english_sentences[i])\n",
        "  print(french_sentences[i])\n",
        "  print('-'*50)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample : 0\n",
            "the association gregory lemarchal\n",
            "\n",
            "l' association gregory lemarchal\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 1\n",
            "after the dresden file\n",
            "\n",
            "après les dossier dresden\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 2\n",
            "the tradition of lanfeust troy\n",
            "invention of reverse auctions\n",
            "\n",
            "la tradition lanfeust de troy\n",
            "invention des ventes aux enchères\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 3\n",
            "the manner of dos passos\n",
            "\n",
            "la manière de dos passos\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 4\n",
            "trace of fat in this book\n",
            "\n",
            "trace de gras dans le livre\n",
            "\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcTWq6nvOmDQ"
      },
      "source": [
        "# ***Etape 2 : Stopwords***\n",
        "première manipulation souvent effectuée dans le traitement du texte est la suppression de ce qu'on appelle en anglais les stopwords. Ce sont les mots très courants dans la langue étudiée (\"et\", \"à\", \"le\"... en français) qui n'apportent pas de valeur informative pour la compréhension du \"sens\" d'un document et corpus. Il sont très fréquents et ralentissent notre travail : nous souhaitons donc les supprimer.\n",
        "\n",
        "Il existe dans la librairie NLTK une liste par défaut des stopwords dans plusieurs Langues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0hrikyVlnAw",
        "outputId": "3077f125-23f2-4282-def4-2169da9a60ea"
      },
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#pour supprimer les stopword de chaque langue\n",
        "def remove_stopwords(text,langue):\n",
        "    STOPWORDS = set(stopwords.words(langue))\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "#supprimer les stopword dans une liste de phrase\n",
        "def remove(sentences,langue):\n",
        "    values=[]\n",
        "    for text in sentences:\n",
        "       values.append(remove_stopwords(text,langue))\n",
        "    return values\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FFU2kwwlp3r"
      },
      "source": [
        "\n",
        "english_sentences=remove(english_sentences,'english')\n",
        "french_sentences=remove(french_sentences,'french')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gth9wlQr9j3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84efca00-455d-4ea0-ba9a-22993522f423"
      },
      "source": [
        "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
        "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
        "\n",
        "print('English Vocab:',len(english_words_counter))\n",
        "print('French Vocab:',len(french_words_counter))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocab: 3981\n",
            "French Vocab: 4271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGEo2THGR3hr"
      },
      "source": [
        "# ***Etape 3 :le Stemming*** \n",
        "racinisation(ou stemming en anglais). Cela consiste à ne conserver que la racine des mots étudiés. L'idée étant de supprimer les suffixes, préfixes et autres des mots afin de ne conserver que leur origine. C'est un procédé plus simple que la lemmatisation et plus rapide à effectuer puisqu'on tronque les mots essentiellement contrairement à la lemmatisation qui nécessite d'utiliser un dictionnaire.Dans notre cas, on va effectuer une racinisation parce qu'il n'existe pas de fonction de lemmatisation de corpus français dans NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3gDnA9XoD2Q"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "def stem_words_langue(text,langue):\n",
        "    stemmer2=SnowballStemmer(langue)\n",
        "    return \" \".join([stemmer2.stem(word) for word in text.split()])\n",
        "\n",
        "def stemming_langue(sentences,langue):\n",
        "  values=[]\n",
        "  for text in sentences:\n",
        "    values.append(stem_words_langue(text,langue))\n",
        "  return values\n",
        " "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zds9TdSQEnn"
      },
      "source": [
        "\n",
        "english_sentences=stemming_langue(english_sentences,'english')\n",
        "french_sentences=stemming_langue(french_sentences,'french')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfDrPIYqTiPv"
      },
      "source": [
        "## ***Etape 4 : la tokenisation***\n",
        "La tokenisation est la division du texte brut en petits morceaux de mots ou de phrases, appelés jetons.Chaque phrase prend son sens par les mots qu'elle contient. Ainsi, en analysant les mots présents dans le texte, nous pouvons facilement interpréter le sens du texte. Une fois que nous avons une liste de mots, nous pouvons également utiliser des outils et des méthodes statistiques pour mieux comprendre le texte. Par exemple, nous pouvons utiliser le nombre de mots et la fréquence des mots pour découvrir l'importance d'un mot dans cette phrase ou ce document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP0AOadJ9lCz"
      },
      "source": [
        "def tokenize(x):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(x)\n",
        "  return tokenizer.texts_to_sequences(x), tokenizer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL_rK09M9vJf"
      },
      "source": [
        "def pad(x, length=None):\n",
        "  return pad_sequences(x, maxlen=length, padding='post')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7_d5he_9ywE"
      },
      "source": [
        "def preprocess(x, y):\n",
        "    \"\"\"\n",
        "      Prétraiter x et y\n",
        "    : param x: Feature Liste des phrases\n",
        "    : param y: Label Liste des phrases\n",
        "    : return: Tuple de (pré-traité x, pré-traité y, x tokenizer, y tokenizer)\n",
        "    \"\"\"\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # La fonction sparse_categorical_crossentropy de Keras nécessite que les étiquettes soient en 3 dimensions\n",
        "    # Extension des dimensions\n",
        "    \n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yDJo4gD96lH"
      },
      "source": [
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
        "    preprocess(english_sentences, french_sentences)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5tS4iyl-Lhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce93fe10-6b5c-421a-db16-4971e57e6d87"
      },
      "source": [
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Preprocessed\n",
            "Max English sentence length: 31\n",
            "Max French sentence length: 34\n",
            "English vocabulary size: 3608\n",
            "French vocabulary size: 3691\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxd8wRHV-OlA"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "  index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "  index_to_words[0] = '<PAD>'\n",
        "\n",
        "   # Nous prédisons la sortie pour un mot donné, puis sélectionnons la meilleure réponse\n",
        "   # En sélectionnant cette étiquette, nous énumérons le mot à partir de l'identifiant\n",
        "    \n",
        "  return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkGroinK-g_9"
      },
      "source": [
        "\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzq41ASNU-pX"
      },
      "source": [
        "# ***Etape 5 les Word embedding*** \n",
        "Nous allons utilisée un prétrained model Glove word embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCwMiNIc56t4",
        "outputId": "15b9a966-b38a-4c1a-f32c-0189332841c5"
      },
      "source": [
        "glove_dir = '/content/drive/MyDrive/glove/'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaowQuvlBT7p"
      },
      "source": [
        "embedding_dim = 100\n",
        "max_words = english_vocab_size+1\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in english_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EoAYcFZWPVi"
      },
      "source": [
        "# ***Etape 6 : Model***\n",
        "Nous allons maintenant procedé a la creation de notre model neuronal ,l'entrainer et l'evaluer puis afficher les resulats  à l'aide d'un graphe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJGH57sOBiJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a688da0b-60ef-4caa-972e-7fcca9ea69b0"
      },
      "source": [
        "def DNN_Model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "\n",
        "  learning_rate = 0.005\n",
        "  model = Sequential()             \n",
        "  model.add(Embedding(english_vocab_size, 100, weights=[embedding_matrix],input_length=input_shape[1], input_shape=input_shape[1:]))\n",
        "  model.add(Dense(32, activation=tf.nn.relu ))\n",
        "  model.add(Dense(32, activation=tf.nn.relu))\n",
        "  model.add(Dense(32, activation=tf.nn.relu))\n",
        "  model.add(Dense(french_vocab_size))\n",
        "  model.summary()\n",
        "  model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "DNN_Model = DNN_Model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "history=DNN_Model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=200, validation_split=0.2)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 34, 100)           360900    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 34, 32)            3232      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 34, 32)            1056      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 34, 32)            1056      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 34, 3692)          121836    \n",
            "=================================================================\n",
            "Total params: 488,080\n",
            "Trainable params: 488,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "2/2 [==============================] - 21s 3s/step - loss: 8.5747 - accuracy: 0.6960 - val_loss: 5.6969 - val_accuracy: 0.0023\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 4.9853 - accuracy: 0.1534 - val_loss: 2.7419 - val_accuracy: 0.8333\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 2.2932 - accuracy: 0.8493 - val_loss: 2.1989 - val_accuracy: 0.8334\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.8357 - accuracy: 0.8497 - val_loss: 2.1828 - val_accuracy: 0.8334\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.8614 - accuracy: 0.8480 - val_loss: 2.2065 - val_accuracy: 0.8334\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.8946 - accuracy: 0.8476 - val_loss: 2.2191 - val_accuracy: 0.8334\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.8873 - accuracy: 0.8476 - val_loss: 2.1818 - val_accuracy: 0.8334\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.8247 - accuracy: 0.8489 - val_loss: 2.1637 - val_accuracy: 0.8335\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.8037 - accuracy: 0.8490 - val_loss: 2.1590 - val_accuracy: 0.8335\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.7980 - accuracy: 0.8482 - val_loss: 2.1389 - val_accuracy: 0.8336\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.7699 - accuracy: 0.8492 - val_loss: 2.1027 - val_accuracy: 0.8346\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.7318 - accuracy: 0.8514 - val_loss: 2.0201 - val_accuracy: 0.8386\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.6411 - accuracy: 0.8573 - val_loss: 1.8668 - val_accuracy: 0.8398\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.5138 - accuracy: 0.8597 - val_loss: 1.7227 - val_accuracy: 0.8367\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.4256 - accuracy: 0.8576 - val_loss: 1.6217 - val_accuracy: 0.8329\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3659 - accuracy: 0.8536 - val_loss: 1.5769 - val_accuracy: 0.8322\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3637 - accuracy: 0.8518 - val_loss: 1.5233 - val_accuracy: 0.8313\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3178 - accuracy: 0.8512 - val_loss: 1.5241 - val_accuracy: 0.8304\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3219 - accuracy: 0.8494 - val_loss: 1.5286 - val_accuracy: 0.8303\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3330 - accuracy: 0.8488 - val_loss: 1.5250 - val_accuracy: 0.8301\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3182 - accuracy: 0.8493 - val_loss: 1.5184 - val_accuracy: 0.8299\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3296 - accuracy: 0.8482 - val_loss: 1.5114 - val_accuracy: 0.8299\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3330 - accuracy: 0.8477 - val_loss: 1.4977 - val_accuracy: 0.8297\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3207 - accuracy: 0.8489 - val_loss: 1.4788 - val_accuracy: 0.8297\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3363 - accuracy: 0.8469 - val_loss: 1.4796 - val_accuracy: 0.8297\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3246 - accuracy: 0.8480 - val_loss: 1.4795 - val_accuracy: 0.8297\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3301 - accuracy: 0.8474 - val_loss: 1.4804 - val_accuracy: 0.8297\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 6s 3s/step - loss: 1.3260 - accuracy: 0.8479 - val_loss: 1.4803 - val_accuracy: 0.8296\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3402 - accuracy: 0.8462 - val_loss: 1.4811 - val_accuracy: 0.8296\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3358 - accuracy: 0.8469 - val_loss: 1.4802 - val_accuracy: 0.8296\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3333 - accuracy: 0.8470 - val_loss: 1.4795 - val_accuracy: 0.8296\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 6s 3s/step - loss: 1.3275 - accuracy: 0.8477 - val_loss: 1.4797 - val_accuracy: 0.8296\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3330 - accuracy: 0.8469 - val_loss: 1.4793 - val_accuracy: 0.8296\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3203 - accuracy: 0.8483 - val_loss: 1.4821 - val_accuracy: 0.8296\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3221 - accuracy: 0.8481 - val_loss: 1.4890 - val_accuracy: 0.8296\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3367 - accuracy: 0.8464 - val_loss: 1.4914 - val_accuracy: 0.8296\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3428 - accuracy: 0.8458 - val_loss: 1.4930 - val_accuracy: 0.8295\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3327 - accuracy: 0.8468 - val_loss: 1.4944 - val_accuracy: 0.8295\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3307 - accuracy: 0.8470 - val_loss: 1.4944 - val_accuracy: 0.8295\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3283 - accuracy: 0.8473 - val_loss: 1.4942 - val_accuracy: 0.8295\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3311 - accuracy: 0.8470 - val_loss: 1.4938 - val_accuracy: 0.8294\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3348 - accuracy: 0.8465 - val_loss: 1.4927 - val_accuracy: 0.8294\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3431 - accuracy: 0.8456 - val_loss: 1.4896 - val_accuracy: 0.8294\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3331 - accuracy: 0.8467 - val_loss: 1.4861 - val_accuracy: 0.8294\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3359 - accuracy: 0.8463 - val_loss: 1.4829 - val_accuracy: 0.8294\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3338 - accuracy: 0.8465 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3363 - accuracy: 0.8464 - val_loss: 1.4797 - val_accuracy: 0.8294\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3363 - accuracy: 0.8465 - val_loss: 1.4797 - val_accuracy: 0.8294\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3310 - accuracy: 0.8470 - val_loss: 1.4798 - val_accuracy: 0.8294\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3292 - accuracy: 0.8473 - val_loss: 1.4798 - val_accuracy: 0.8294\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3321 - accuracy: 0.8468 - val_loss: 1.4799 - val_accuracy: 0.8294\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3317 - accuracy: 0.8469 - val_loss: 1.4799 - val_accuracy: 0.8294\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3415 - accuracy: 0.8458 - val_loss: 1.4799 - val_accuracy: 0.8294\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3396 - accuracy: 0.8460 - val_loss: 1.4799 - val_accuracy: 0.8294\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3348 - accuracy: 0.8465 - val_loss: 1.4799 - val_accuracy: 0.8294\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3388 - accuracy: 0.8462 - val_loss: 1.4776 - val_accuracy: 0.8294\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3323 - accuracy: 0.8469 - val_loss: 1.4789 - val_accuracy: 0.8294\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3357 - accuracy: 0.8464 - val_loss: 1.4801 - val_accuracy: 0.8294\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3310 - accuracy: 0.8470 - val_loss: 1.4812 - val_accuracy: 0.8294\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3283 - accuracy: 0.8474 - val_loss: 1.4815 - val_accuracy: 0.8294\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3335 - accuracy: 0.8467 - val_loss: 1.4814 - val_accuracy: 0.8294\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3312 - accuracy: 0.8470 - val_loss: 1.4807 - val_accuracy: 0.8294\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3270 - accuracy: 0.8475 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3318 - accuracy: 0.8469 - val_loss: 1.4800 - val_accuracy: 0.8294\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3285 - accuracy: 0.8472 - val_loss: 1.4800 - val_accuracy: 0.8294\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3355 - accuracy: 0.8465 - val_loss: 1.4802 - val_accuracy: 0.8294\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3323 - accuracy: 0.8468 - val_loss: 1.4800 - val_accuracy: 0.8294\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3271 - accuracy: 0.8474 - val_loss: 1.4791 - val_accuracy: 0.8294\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3340 - accuracy: 0.8467 - val_loss: 1.4791 - val_accuracy: 0.8294\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3408 - accuracy: 0.8459 - val_loss: 1.4791 - val_accuracy: 0.8294\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3363 - accuracy: 0.8464 - val_loss: 1.4792 - val_accuracy: 0.8294\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3288 - accuracy: 0.8473 - val_loss: 1.4792 - val_accuracy: 0.8294\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3361 - accuracy: 0.8464 - val_loss: 1.4792 - val_accuracy: 0.8294\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3350 - accuracy: 0.8467 - val_loss: 1.4792 - val_accuracy: 0.8294\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3236 - accuracy: 0.8479 - val_loss: 1.4793 - val_accuracy: 0.8294\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3225 - accuracy: 0.8480 - val_loss: 1.4793 - val_accuracy: 0.8294\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3373 - accuracy: 0.8463 - val_loss: 1.4793 - val_accuracy: 0.8294\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3318 - accuracy: 0.8471 - val_loss: 1.4793 - val_accuracy: 0.8294\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 6s 3s/step - loss: 1.3362 - accuracy: 0.8464 - val_loss: 1.4793 - val_accuracy: 0.8294\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3360 - accuracy: 0.8464 - val_loss: 1.4793 - val_accuracy: 0.8294\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3335 - accuracy: 0.8468 - val_loss: 1.4776 - val_accuracy: 0.8294\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3344 - accuracy: 0.8466 - val_loss: 1.4797 - val_accuracy: 0.8294\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3400 - accuracy: 0.8460 - val_loss: 1.4813 - val_accuracy: 0.8294\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3283 - accuracy: 0.8472 - val_loss: 1.4823 - val_accuracy: 0.8294\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3359 - accuracy: 0.8464 - val_loss: 1.4818 - val_accuracy: 0.8294\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3354 - accuracy: 0.8466 - val_loss: 1.4813 - val_accuracy: 0.8294\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3349 - accuracy: 0.8466 - val_loss: 1.4814 - val_accuracy: 0.8294\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3316 - accuracy: 0.8469 - val_loss: 1.4817 - val_accuracy: 0.8294\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3398 - accuracy: 0.8459 - val_loss: 1.4816 - val_accuracy: 0.8294\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3322 - accuracy: 0.8468 - val_loss: 1.4815 - val_accuracy: 0.8294\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3371 - accuracy: 0.8463 - val_loss: 1.4816 - val_accuracy: 0.8294\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3313 - accuracy: 0.8470 - val_loss: 1.4815 - val_accuracy: 0.8294\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3386 - accuracy: 0.8461 - val_loss: 1.4812 - val_accuracy: 0.8294\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3416 - accuracy: 0.8459 - val_loss: 1.4817 - val_accuracy: 0.8294\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3385 - accuracy: 0.8461 - val_loss: 1.4827 - val_accuracy: 0.8294\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3341 - accuracy: 0.8466 - val_loss: 1.4830 - val_accuracy: 0.8294\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3421 - accuracy: 0.8456 - val_loss: 1.4826 - val_accuracy: 0.8294\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3277 - accuracy: 0.8473 - val_loss: 1.4818 - val_accuracy: 0.8294\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3352 - accuracy: 0.8465 - val_loss: 1.4800 - val_accuracy: 0.8294\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3248 - accuracy: 0.8476 - val_loss: 1.4778 - val_accuracy: 0.8294\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3402 - accuracy: 0.8459 - val_loss: 1.4799 - val_accuracy: 0.8294\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3404 - accuracy: 0.8459 - val_loss: 1.4799 - val_accuracy: 0.8294\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3357 - accuracy: 0.8463 - val_loss: 1.4799 - val_accuracy: 0.8294\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3296 - accuracy: 0.8472 - val_loss: 1.4800 - val_accuracy: 0.8294\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3354 - accuracy: 0.8465 - val_loss: 1.4800 - val_accuracy: 0.8294\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3397 - accuracy: 0.8460 - val_loss: 1.4800 - val_accuracy: 0.8294\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3475 - accuracy: 0.8451 - val_loss: 1.4800 - val_accuracy: 0.8294\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3450 - accuracy: 0.8454 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3336 - accuracy: 0.8466 - val_loss: 1.4804 - val_accuracy: 0.8294\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3384 - accuracy: 0.8461 - val_loss: 1.4798 - val_accuracy: 0.8294\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3328 - accuracy: 0.8468 - val_loss: 1.4777 - val_accuracy: 0.8294\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3390 - accuracy: 0.8461 - val_loss: 1.4801 - val_accuracy: 0.8294\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3380 - accuracy: 0.8462 - val_loss: 1.4801 - val_accuracy: 0.8294\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3384 - accuracy: 0.8462 - val_loss: 1.4801 - val_accuracy: 0.8294\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3403 - accuracy: 0.8458 - val_loss: 1.4778 - val_accuracy: 0.8294\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3281 - accuracy: 0.8474 - val_loss: 1.4780 - val_accuracy: 0.8294\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3405 - accuracy: 0.8458 - val_loss: 1.4798 - val_accuracy: 0.8294\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3334 - accuracy: 0.8467 - val_loss: 1.4796 - val_accuracy: 0.8294\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3365 - accuracy: 0.8463 - val_loss: 1.4796 - val_accuracy: 0.8294\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3389 - accuracy: 0.8460 - val_loss: 1.4796 - val_accuracy: 0.8294\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3349 - accuracy: 0.8465 - val_loss: 1.4797 - val_accuracy: 0.8294\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3360 - accuracy: 0.8464 - val_loss: 1.4789 - val_accuracy: 0.8294\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3437 - accuracy: 0.8455 - val_loss: 1.4789 - val_accuracy: 0.8294\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3406 - accuracy: 0.8459 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3376 - accuracy: 0.8463 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3374 - accuracy: 0.8461 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3399 - accuracy: 0.8460 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3309 - accuracy: 0.8470 - val_loss: 1.4787 - val_accuracy: 0.8294\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3430 - accuracy: 0.8457 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3410 - accuracy: 0.8459 - val_loss: 1.4780 - val_accuracy: 0.8294\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3395 - accuracy: 0.8459 - val_loss: 1.4830 - val_accuracy: 0.8294\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3315 - accuracy: 0.8469 - val_loss: 1.4838 - val_accuracy: 0.8294\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3317 - accuracy: 0.8468 - val_loss: 1.4810 - val_accuracy: 0.8294\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3366 - accuracy: 0.8463 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3434 - accuracy: 0.8455 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3366 - accuracy: 0.8463 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3354 - accuracy: 0.8464 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3276 - accuracy: 0.8474 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3431 - accuracy: 0.8455 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3347 - accuracy: 0.8464 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3397 - accuracy: 0.8459 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3433 - accuracy: 0.8455 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3311 - accuracy: 0.8470 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3305 - accuracy: 0.8469 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3322 - accuracy: 0.8467 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3436 - accuracy: 0.8455 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 6s 3s/step - loss: 1.3264 - accuracy: 0.8475 - val_loss: 1.4789 - val_accuracy: 0.8294\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3374 - accuracy: 0.8462 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3330 - accuracy: 0.8468 - val_loss: 1.4789 - val_accuracy: 0.8294\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3405 - accuracy: 0.8459 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3451 - accuracy: 0.8454 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3367 - accuracy: 0.8463 - val_loss: 1.4808 - val_accuracy: 0.8294\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3287 - accuracy: 0.8472 - val_loss: 1.4837 - val_accuracy: 0.8294\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3354 - accuracy: 0.8464 - val_loss: 1.4833 - val_accuracy: 0.8294\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3497 - accuracy: 0.8448 - val_loss: 1.4810 - val_accuracy: 0.8294\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3305 - accuracy: 0.8470 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3312 - accuracy: 0.8469 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3354 - accuracy: 0.8464 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3397 - accuracy: 0.8459 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3410 - accuracy: 0.8458 - val_loss: 1.4788 - val_accuracy: 0.8294\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3310 - accuracy: 0.8469 - val_loss: 1.4787 - val_accuracy: 0.8294\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3387 - accuracy: 0.8461 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3355 - accuracy: 0.8464 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3453 - accuracy: 0.8453 - val_loss: 1.4783 - val_accuracy: 0.8294\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3329 - accuracy: 0.8466 - val_loss: 1.4783 - val_accuracy: 0.8294\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3314 - accuracy: 0.8469 - val_loss: 1.4783 - val_accuracy: 0.8294\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3353 - accuracy: 0.8465 - val_loss: 1.4783 - val_accuracy: 0.8294\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3417 - accuracy: 0.8458 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3383 - accuracy: 0.8461 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3391 - accuracy: 0.8459 - val_loss: 1.4783 - val_accuracy: 0.8294\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3310 - accuracy: 0.8469 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3386 - accuracy: 0.8460 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3371 - accuracy: 0.8463 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3369 - accuracy: 0.8463 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3298 - accuracy: 0.8471 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3362 - accuracy: 0.8464 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3342 - accuracy: 0.8467 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3394 - accuracy: 0.8460 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3433 - accuracy: 0.8456 - val_loss: 1.4784 - val_accuracy: 0.8294\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3357 - accuracy: 0.8464 - val_loss: 1.4785 - val_accuracy: 0.8294\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3329 - accuracy: 0.8468 - val_loss: 1.4785 - val_accuracy: 0.8294\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3309 - accuracy: 0.8470 - val_loss: 1.4785 - val_accuracy: 0.8294\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3352 - accuracy: 0.8465 - val_loss: 1.4785 - val_accuracy: 0.8294\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.3307 - accuracy: 0.8469 - val_loss: 1.4785 - val_accuracy: 0.8294\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3316 - accuracy: 0.8468 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3365 - accuracy: 0.8464 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3345 - accuracy: 0.8467 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3393 - accuracy: 0.8459 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3303 - accuracy: 0.8471 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3340 - accuracy: 0.8466 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3257 - accuracy: 0.8476 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3368 - accuracy: 0.8463 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3442 - accuracy: 0.8455 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3356 - accuracy: 0.8464 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3441 - accuracy: 0.8455 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3384 - accuracy: 0.8461 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3274 - accuracy: 0.8474 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3365 - accuracy: 0.8463 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3361 - accuracy: 0.8463 - val_loss: 1.4786 - val_accuracy: 0.8294\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3365 - accuracy: 0.8463 - val_loss: 1.4786 - val_accuracy: 0.8294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz9MWafhZ8pE",
        "outputId": "686362c8-294f-4c07-b501-e48e3f294725"
      },
      "source": [
        "scores =  DNN_Model.evaluate(tmp_x, preproc_french_sentences, verbose=0)\n",
        "# Displays the accuracy of correct sentiment prediction over test data\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 84.29%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "_lrSLLX83I20",
        "outputId": "b3dc82c6-333b-4d32-b18c-a9d0304a158d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3gU9d338feXcIgIxXKwIqdERREf5RSxYrX4lD6CWijeWI3RQm1vBPW22lofLFYpynPVaquXl4c2XtYDxgLWluJdrC3W061ViRhAEDRq0CBYGhVQRBL4PX/MbNjs7CabZLObX/i8rmuvnfnNb2e+O7v5ZHZmdsecc4iIiP865boAERHJDAW6iEgHoUAXEekgFOgiIh2EAl1EpINQoIuIdBAK9A7MzJ4ws+mZ7ptLZlZlZhPaYL7OzI4Kh39jZj9Lp28LllNiZn9raZ0ijTGdh96+mNmncaPdgS+AveH4Jc65suxX1X6YWRXwA+fcigzP1wFDnXOVmeprZgXAu0AX51xdJuoUaUznXBcgDTnnesSGGwsvM+uskJD2Qu/H9kG7XDxhZuPNrNrM/q+ZbQXuN7Mvm9l/m9k2M/s4HB4Y95hnzOwH4fAMM/sfM7s17PuumU1qYd9CM3vOzHaa2Qozu8vMHk5Rdzo13mhmL4Tz+5uZ9Y2bfpGZbTKzGjOb28j6OcnMtppZXlzbVDNbEw6PNbN/mtknZrbFzO40s64p5vWAmd0UN/6T8DEfmNnFCX3PMrPXzGyHmb1vZvPiJj8X3n9iZp+a2cmxdRv3+HFmttLMtof349JdN81cz73N7P7wOXxsZkvjpk0xs4rwObxtZhPD9ga7t8xsXux1NrOCcNfT983sPeAfYfuj4euwPXyPHBf3+IPM7Ffh67k9fI8dZGZ/MbP/Sng+a8xsarLnKqkp0P1yGNAbGALMJHj97g/HBwOfA3c28viTgI1AX+CXwH1mZi3o+wjwCtAHmAdc1Mgy06nxAuB7wKFAV+BqADMbDtwTzv/wcHkDScI59zLwGfC/E+b7SDi8F7gqfD4nA98ALm2kbsIaJob1fBMYCiTuv/8M+C5wCHAWMNvMvh1OOy28P8Q518M598+EefcG/gLcET63XwN/MbM+Cc8hsm6SaGo9LyTYhXdcOK/bwhrGAg8BPwmfw2lAVar1kcTXgWOBM8LxJwjW06HAKiB+F+GtwBhgHMH7+BpgH/AgcGGsk5mNAAYQrBtpDuecbu30RvCHNSEcHg/sAfIb6T8S+Dhu/BmCXTYAM4DKuGndAQcc1py+BGFRB3SPm/4w8HCazylZjdfFjV8K/DUcvh5YFDft4HAdTEgx75uA34XDPQnCdkiKvlcCf4obd8BR4fADwE3h8O+AX8T1Ozq+b5L53g7cFg4XhH07x02fAfxPOHwR8ErC4/8JzGhq3TRnPQP9CYLzy0n6/TZWb2Pvv3B8Xux1jntuRzRSwyFhn14E/3A+B0Yk6ZcPfExwXAKC4L87239vHeGmLXS/bHPO7Y6NmFl3M/tt+BF2B8FH/EPidzsk2BobcM7tCgd7NLPv4cBHcW0A76cqOM0at8YN74qr6fD4eTvnPgNqUi2LYGv8HDPrBpwDrHLObQrrODrcDbE1rOP/EWytN6VBDcCmhOd3kpk9He7q2A7MSnO+sXlvSmjbRLB1GpNq3TTQxHoeRPCafZzkoYOAt9OsN5n6dWNmeWb2i3C3zQ72b+n3DW/5yZYVvqcXAxeaWSegmOAThTSTAt0viack/Rg4BjjJOfcl9n/ET7UbJRO2AL3NrHtc26BG+remxi3x8w6X2SdVZ+fceoJAnETD3S0Q7LrZQLAV+CXgpy2pgeATSrxHgGXAIOdcL+A3cfNt6hSyDwh2kcQbDGxOo65Eja3n9wles0OSPO594MgU8/yM4NNZzGFJ+sQ/xwuAKQS7pXoRbMXHavg3sLuRZT0IlBDsCtvlEnZPSXoU6H7rSfAx9pNwf+wNbb3AcIu3HJhnZl3N7GTgW21U4x+As83sa+EBzPk0/Z59BPghQaA9mlDHDuBTMxsGzE6zhiXADDMbHv5DSay/J8HW7+5wf/QFcdO2EezqOCLFvJcDR5vZBWbW2czOA4YD/51mbYl1JF3PzrktBPu27w4PnnYxs1jg3wd8z8y+YWadzGxAuH4AKoDzw/5FwLQ0aviC4FNUd4JPQbEa9hHsvvq1mR0ebs2fHH6aIgzwfcCv0NZ5iynQ/XY7cBDB1s9LwF+ztNwSggOLNQT7rRcT/CEn0+IanXPrgMsIQnoLwX7W6iYe9nuCA3X/cM79O679aoKw3QncG9acTg1PhM/hH0BleB/vUmC+me0k2Oe/JO6xu4AFwAsWnF3z1YR51wBnE2xd1xAcJDw7oe50NbWeLwJqCT6l/IvgGALOuVcIDrreBmwHnmX/p4afEWxRfwz8nIafeJJ5iOAT0mZgfVhHvKuBtcBK4CPgZhpm0EPA8QTHZKQF9MUiaTUzWwxscM61+ScE6bjM7LvATOfc13Jdi6+0hS7NZmYnmtmR4Uf0iQT7TZc29TiRVMLdWZcCpbmuxWcKdGmJwwhOqfuU4Bzq2c6513JakXjLzM4gON7wIU3v1pFGaJeLiEgHoS10EZEOImc/ztW3b19XUFCQq8WLiHjp1Vdf/bdzrl+yaTkL9IKCAsrLy3O1eBERL5lZ4reL62mXi4hIB6FAFxHpIBToIiIdhAJdRKSDUKCLiHQQB3Sgl5VBQQF06hTclx3Ql18WEd8dsIF+6aVw0UWwaRM4F9xfeCH07atgFxE/HXCBXlYWhPY99wRBnqimJgh2s+CmgBcRX+Tsi0WtVVYGc+cGW9ZmycO5UyfYty/19HTEAv7CC1P3ycRy0qHlaDlajv/LiU0fMgQWLICSkswt28tALyuDmTNhV3hVy1Qvzr59jU/PFC1Hy9FytJx0lxObvmlTkGOQuVD3cpfL3Ln7w1xExFe7dgV5lileBvp77+W6AhGRzMhknnkZ6IMTr7veSl26ZHZ+IiLpymSeeRnoCxZAfn7m5pefD+edB/37Z26eIiJN6d49yLNM8TLQS0rgoINaN4/Bg+Hhh2HtWhg/HhYvhi1bgmn5+VBYCEOHQteurS43Y8y0HC1Hy/F9ObHpQ4ZAaanOcgGCI8UXXgjXXhusoE6d9p87nmzcDHr1Cm6Jli2DVatgwwbYujUI9titc2f47DOorYU9exresn31vmwtT8vRcrSctluOc8H3YGbNyvyyvQ70fv1g+PDMzG/06ODW1lr6pmrJ47Ss3D1Oy8rd43xY1sEHt+xxTfE60Dt5uMOopR/7svVxUUT85WEkBnwNdBGRtpJWJJrZRDPbaGaVZjYnyfTBZva0mb1mZmvM7MzMl9qQAl1EpKEmI9HM8oC7gEnAcKDYzBL3XF8HLHHOjQLOB+7OdKGJFOgiIg2lE4ljgUrn3DvOuT3AImBKQh8HfCkc7gV8kLkSk1Ogi4g0lE4kDgDejxuvDtvizQMuNLNqYDnwX8lmZGYzzazczMq3bdvWgnL3U6CLiDSUqUgsBh5wzg0EzgQWmllk3s65UudckXOuqF+/fi1emHPBTYEuIrJfOpG4GRgUNz4wbIv3fWAJgHPun0A+0DcTBSYTO/czL6+tliAi4p90An0lMNTMCs2sK8FBz2UJfd4DvgFgZscSBHrr9qk0IvZ7wtpCFxHZr8lIdM7VAZcDTwJvEJzNss7M5pvZ5LDbj4H/NLPVwO+BGc613RdtFegiIlFpfVPUObec4GBnfNv1ccPrgVMyW1pqCnQRkSgvI1GBLiIS5WUkKtBFRKK8jEQFuohIlJeRqEAXEYnyMhIV6CIiUV5G4t69wb0CXURkPy8jUVvoIiJRXkaiAl1EJMrLSFSgi4hEeRmJCnQRkSgvI1GBLiIS5WUkKtBFRKK8jEQFuohIlJeRqEAXEYnyMhIV6CIiUV5GogJdRCTKy0hUoIuIRHkZiQp0EZEoLyNRgS4iEuVlJCrQRUSivIzEWKDn5eW2DhGR9sTrQNcWuojIfl5GogJdRCTKy0hUoIuIRHkZiQp0EZEoLyNRgS4iEuVlJCrQRUSivIzEvXuDewW6iMh+XkaittBFRKK8jEQFuohIlJeRqEAXEYnyMhIV6CIiUV5GogJdRCTKq0gsK4OCAvjWt4Lxv/41p+WIiLQr3gR6WRnMnAmbNu1vW7AgaBcREY8Cfe5c2LWrYdvu3UG7iIh4FOjvvde8dhGRA01agW5mE81so5lVmtmcFH2+Y2brzWydmT2S2TJh8ODmtYuIHGiaDHQzywPuAiYBw4FiMxue0GcocC1winPuOODKTBe6YAF0796wLT8/aBcRkfS20McClc65d5xze4BFwJSEPv8J3OWc+xjAOfevzJYJJSVQWgpDhuxvu/HGoF1ERNIL9AHA+3Hj1WFbvKOBo83sBTN7ycwmJpuRmc00s3IzK9+2bVuziy0pgaoqWLgwGP/2t5s9CxGRDitTB0U7A0OB8UAxcK+ZHZLYyTlX6pwrcs4V9evXr8UL0xeLRESi0onEzcCguPGBYVu8amCZc67WOfcu8CZBwLcJBbqISFQ6kbgSGGpmhWbWFTgfWJbQZynB1jlm1pdgF8w7GayzAQW6iEhUk5HonKsDLgeeBN4Aljjn1pnZfDObHHZ7Eqgxs/XA08BPnHM1bVV0LNDz8tpqCSIi/umcTifn3HJgeULb9XHDDvhReGtz2kIXEYnyMhIV6CIiUV5GogJdRCTKy0hUoIuIRHkZiQp0EZEoLyNx797gXoEuIrKfl5GoLXQRkSgvI1GBLiIS5WUkKtBFRKK8jEQFuohIlJeRqEAXEYnyMhIV6CIiUV5GogJdRCTKy0iMBbpZbusQEWlPvA10bZ2LiDTkZSwq0EVEoryMRQW6iEiUl7GoQBcRifIyFhXoIiJRXsaiAl1EJMrLWFSgi4hEeRmLCnQRkSgvY1GBLiIS5WUs7tsHeXm5rkJEpH3xNtC1hS4i0pCXsahAFxGJ8jIWFegiIlFexuLevQp0EZFEXsaittBFRKK8jEUFuohIlJexqEAXEYnyMhYV6CIiUV7GogJdRCTKy1hUoIuIRHkZiwp0EZEoL2NRgS4iEuVlLCrQRUSivIxFBbqISFRasWhmE81so5lVmtmcRvr9h5k5MyvKXIlRCnQRkagmY9HM8oC7gEnAcKDYzIYn6dcT+CHwcqaLTKRAFxGJSicWxwKVzrl3nHN7gEXAlCT9bgRuBnZnsL6kFOgiIlHpxOIA4P248eqwrZ6ZjQYGOef+0tiMzGymmZWbWfm2bduaXWyMAl1EJKrVsWhmnYBfAz9uqq9zrtQ5V+ScK+rXr1+Ll6lAFxGJSicWNwOD4sYHhm0xPYH/BTxjZlXAV4FlbXlgVIEuIhKVTiyuBIaaWaGZdQXOB5bFJjrntjvn+jrnCpxzBcBLwGTnXHmbVIwCXUQkmSZj0TlXB1wOPAm8ASxxzq0zs/lmNrmtC0xGgS4iEtU5nU7OueXA8oS261P0Hd/6shqnQBcRifIyFvftg7y8XFchItK+eBvo2kIXEWnIy1hUoIuIRHkZi3v3KtBFRBJ5GYvaQhcRifIyFhXoIiJRXsaiAl1EJMrLWFSgi4hEeRmLCnQRkSgvY1GBLiIS5WUsKtBFRKK8jEUFuohIlJexqEAXEYnyMhYV6CIiUV7GogJdRCTKy1hUoIuIRHkZiwp0EZEoL2NRgS4iEuVlLCrQRUSivIxFBbqISJSXsahAFxGJ8jIWFegiIlFexqICXUQkystYVKCLiER5GYv79kFeXq6rEBFpX7wNdG2hi4g05GUs7t2rQBcRSeRlLGoLXUQkyrtYdC64KdBFRBryLhadC+4V6CIiDXkXi/v2BfcKdBGRhryLxVig/+pXQagXFEBZWU5LEhFpFzrnuoDmioX3J58E95s2wcyZwXBJSW5qEhFpD7zbQr/hhmjbrl0wd272axERaU+8C/T330/e/t572a1DRKS98S7QBw1K3j54cHbrEBFpb7wL9Ouui7Z17w4LFmS/FhGR9iStQDeziWa20cwqzWxOkuk/MrP1ZrbGzJ4ysyGZLzUwbVpw/+UvgxkMGQKlpTogKiLSZKCbWR5wFzAJGA4Um9nwhG6vAUXOuROAPwC/zHShMbHTFufPD4arqhTmIiKQ3hb6WKDSOfeOc24PsAiYEt/BOfe0c25XOPoSMDCzZe6nLxaJiCSXTiwOAOLPLakO21L5PvBEsglmNtPMys2sfNu2belXGUeBLiKSXEZj0cwuBIqAW5JNd86VOueKnHNF/fr1a9EyFOgiIsml803RzUD8yYIDw7YGzGwCMBf4unPui8yUF6VAFxFJLp1YXAkMNbNCM+sKnA8si+9gZqOA3wKTnXP/ynyZ+ynQRUSSazIWnXN1wOXAk8AbwBLn3Dozm29mk8NutwA9gEfNrMLMlqWYXasp0EVEkkvrx7mcc8uB5Qlt18cNT8hwXSkp0EVEkvMuFhXoIiLJeReLsUDPy8ttHSIi7Y13gb53b3CvLXQRkYa8i0XtchERSc67WFSgi4gk510sKtBFRJLzLhYV6CIiyXkXiwp0EZHkvItFBbqISHLexaICXUQkOe9iUYEuIpKcd7GoQBcRSS6tH+dqTxToIq1XW1tLdXU1u3fvznUpkkJ+fj4DBw6kS5cuaT9GgS5yAKqurqZnz54UFBRgZrkuRxI456ipqaG6uprCwsK0H+ddLCrQRVpv9+7d9OnTR2HeTpkZffr0afYnKO9iUYEukhkK8/atJa+Pd7GoQBcRSc67WFSgi2RfWRkUFAR/dwUFwXhr1NTUMHLkSEaOHMlhhx3GgAED6sf37NnT6GPLy8u54oormlzGuHHjWlekh3RQVEQaVVYGM2fCrl3B+KZNwThASUnL5tmnTx8qKioAmDdvHj169ODqq6+un15XV0fnzsnjqaioiKKioiaX8eKLL7asOI95F4sKdJHsmjt3f5jH7NoVtGfSjBkzmDVrFieddBLXXHMNr7zyCieffDKjRo1i3LhxbNy4EYBnnnmGs88+Gwj+GVx88cWMHz+eI444gjvuuKN+fj169KjvP378eKZNm8awYcMoKSnBOQfA8uXLGTZsGGPGjOGKK66on2+8qqoqTj31VEaPHs3o0aMb/KO4+eabOf744xkxYgRz5swBoLKykgkTJjBixAhGjx7N22+/ndkV1QhtoYtIo957r3ntrVFdXc2LL75IXl4eO3bs4Pnnn6dz586sWLGCn/70pzz22GORx2zYsIGnn36anTt3cswxxzB79uzIuduvvfYa69at4/DDD+eUU07hhRdeoKioiEsuuYTnnnuOwsJCiouLk9Z06KGH8ve//538/HzeeustiouLKS8v54knnuDPf/4zL7/8Mt27d+ejjz4CoKSkhDlz5jB16lR2797NvlhoZYECXUQaNXhwsJslWXumnXvuueSFFwzevn0706dP56233sLMqK2tTfqYs846i27dutGtWzcOPfRQPvzwQwYOHNigz9ixY+vbRo4cSVVVFT169OCII46oP8+7uLiY0tLSyPxra2u5/PLLqaioIC8vjzfffBOAFStW8L3vfY/u3bsD0Lt3b3bu3MnmzZuZOnUqEHw5KJu8i0UFukh2LVgAYWbV6949aM+0gw8+uH74Zz/7Gaeffjqvv/46jz/+eMpzsrt161Y/nJeXR11dXYv6pHLbbbfxla98hdWrV1NeXt7kQdtc8i4WFegi2VVSAqWlMGQImAX3paUtPyCaru3btzNgwAAAHnjggYzP/5hjjuGdd96hqqoKgMWLF6eso3///nTq1ImFCxeyN7xS/Te/+U3uv/9+doUHGD766CN69uzJwIEDWbp0KQBffPFF/fRs8C4WFegi2VdSAlVVwd9fVVXbhznANddcw7XXXsuoUaOatUWdroMOOoi7776biRMnMmbMGHr27EmvXr0i/S699FIefPBBRowYwYYNG+o/RUycOJHJkydTVFTEyJEjufXWWwFYuHAhd9xxByeccALjxo1j69atGa89FYsd7c22oqIiV15e3uzHLVoExcXwxhswbFgbFCZyAHjjjTc49thjc11Gzn366af06NED5xyXXXYZQ4cO5aqrrsp1WfWSvU5m9qpzLul5m95t52oLXUQy5d5772XkyJEcd9xxbN++nUsuuSTXJbWKznIRkQPWVVdd1a62yFvLu1hUoIuIJOddLCrQRUSS8y4WFegiIsl5F4sKdBGR5LyLRQW6iP9OP/10nnzyyQZtt99+O7Nnz075mPHjxxM71fnMM8/kk08+ifSZN29e/fngqSxdupT169fXj19//fWsWLGiOeW3W97FogJdxH/FxcUsWrSoQduiRYtS/kBWouXLl3PIIYe0aNmJgT5//nwmTJjQonm1NzptUeQAd+WVEP40ecaMHAm33556+rRp07juuuvYs2cPXbt2paqqig8++IBTTz2V2bNns3LlSj7//HOmTZvGz3/+88jjCwoKKC8vp2/fvixYsIAHH3yQQw89lEGDBjFmzBggOMe8tLSUPXv2cNRRR7Fw4UIqKipYtmwZzz77LDfddBOPPfYYN954I2effTbTpk3jqaee4uqrr6auro4TTzyRe+65h27dulFQUMD06dN5/PHHqa2t5dFHH2VYwjcbq6qquOiii/jss88AuPPOO+svsnHzzTfz8MMP06lTJyZNmsQvfvELKisrmTVrFtu2bSMvL49HH32UI488slXr3btYVKCL+K93796MHTuWJ554Agi2zr/zne9gZixYsIDy8nLWrFnDs88+y5o1a1LO59VXX2XRokVUVFSwfPlyVq5cWT/tnHPOYeXKlaxevZpjjz2W++67j3HjxjF58mRuueUWKioqGgTo7t27mTFjBosXL2bt2rXU1dVxzz331E/v27cvq1atYvbs2Ul368R+ZnfVqlUsXry4/qpK8T+zu3r1aq655hog+Jndyy67jNWrV/Piiy/Sv3//1q1UtIUucsBrbEu6LcV2u0yZMoVFixZx3333AbBkyRJKS0upq6tjy5YtrF+/nhNOOCHpPJ5//nmmTp1a/xO2kydPrp/2+uuvc9111/HJJ5/w6aefcsYZZzRaz8aNGyksLOToo48GYPr06dx1111ceeWVQPAPAmDMmDH88Y9/jDy+PfzMblqxaGYTzWyjmVWa2Zwk07uZ2eJw+stmVpCR6pJQoIt0DFOmTOGpp55i1apV7Nq1izFjxvDuu+9y66238tRTT7FmzRrOOuuslD+b25QZM2Zw5513snbtWm644YYWzycm9hO8qX5+tz38zG6TsWhmecBdwCRgOFBsZsMTun0f+Ng5dxRwG3BzpguNUaCLdAw9evTg9NNP5+KLL64/GLpjxw4OPvhgevXqxYcffli/SyaV0047jaVLl/L555+zc+dOHn/88fppO3fupH///tTW1lIWd1Xrnj17snPnzsi8jjnmGKqqqqisrASCX038+te/nvbzaQ8/s5tOLI4FKp1z7zjn9gCLgCkJfaYAD4bDfwC+YWbW6uqSUKCLdBzFxcWsXr26PtBHjBjBqFGjGDZsGBdccAGnnHJKo48fPXo05513HiNGjGDSpEmceOKJ9dNuvPFGTjrpJE455ZQGBzDPP/98brnlFkaNGtXgep/5+fncf//9nHvuuRx//PF06tSJWbNmpf1c2sPP7Db587lmNg2Y6Jz7QTh+EXCSc+7yuD6vh32qw/G3wz7/TpjXTGAmwODBg8dsSnZdqyYsWwYPPwwPPQRZvrqTSIehn8/1Q7v++VznXKlzrsg5V9SvX78WzWPyZFiyRGEuIpIonUDfDAyKGx8YtiXtY2adgV5ATSYKFBGR9KQT6CuBoWZWaGZdgfOBZQl9lgHTw+FpwD9cri6FJCJp0Z9o+9aS16fJQHfO1QGXA08CbwBLnHPrzGy+mcVO+rwP6GNmlcCPgMipjSLSfuTn51NTU6NQb6ecc9TU1DT7/HTvrikqIq1XW1tLdXV1q8/NlraTn5/PwIED6dKlS4P2xg6KevdNURFpvS5dulBYWJjrMiTDdDa3iEgHoUAXEekgFOgiIh1Ezg6Kmtk2oPlfFQ30Bf7dZK/caK+1qa7mUV3N115r62h1DXHOJf1mZs4CvTXMrDzVUd5ca6+1qa7mUV3N115rO5Dq0i4XEZEOQoEuItJB+BropbkuoBHttTbV1Tyqq/naa20HTF1e7kMXEZEoX7fQRUQkgQJdRKSD8C7Qm7pgdRbrGGRmT5vZejNbZ2Y/DNvnmdlmM6sIb2fmoLYqM1sbLr88bOttZn83s7fC+y9nuaZj4tZJhZntMLMrc7W+zOx3Zvav8Gpbsbak68gCd4TvuTVmNjrLdd1iZhvCZf/JzA4J2wvM7PO4dfebLNeV8rUzs2vD9bXRzM5oq7oaqW1xXF1VZlYRtmdlnTWSD237HnPOeXMD8oC3gSOArsBqYHiOaukPjA6HewJvElxEex5wdY7XUxXQN6Htl8CccHgOcHOOX8etwJBcrS/gNGA08HpT6wg4E3gCMOCrwMtZruv/AJ3D4Zvj6iqI75eD9ZX0tQv/DlYD3YDC8G82L5u1JUz/FXB9NtdZI/nQpu8x37bQ07lgdVY457Y451aFwzsJfit+QC5qSVP8hbwfBL6dw1q+AbztnGvpN4VbzTn3HPBRQnOqdTQFeMgFXgIOMbP+2arLOfc3F1yXAOAlgquGZVWK9ZXKFGCRc+4L59y7QCXB327WazMzA74D/L6tlp+iplT50KbvMd8CfQDwftx4Ne0gRM2sABgFvBw2XR5+bPpdtndthBzwNzN71YILcwN8xTm3JRzeCnwlB3XFnE/DP7Bcr6+YVOuoPb3vLibYkospNLPXzOxZMzs1B/Uke+3a0/o6FfjQOfdWXFtW11lCPrTpe8y3QG93zKwH8BhwpXNuB3APcCQwEthC8HEv277mnBsNTAIuM7PT4ie64DNeTs5XteAyhpOBR8Om9rC+InK5jlIxs7lAHVAWNm0BBjvnRhFcKewRM/tSFktql69dgmIabjxkdZ0lyYd6bfEe8y3Q07lgddaYWReCF6vMOfdHAOfch865vc65fcC9tGjGC8QAAAGrSURBVOFHzVScc5vD+38Bfwpr+DD2ES68/1e26wpNAlY55z4Ma8z5+oqTah3l/H1nZjOAs4GSMAgId2nUhMOvEuyrPjpbNTXy2uV8fUH9BevPARbH2rK5zpLlA238HvMt0NO5YHVWhPvm7gPecM79Oq49fr/XVOD1xMe2cV0Hm1nP2DDBAbXXaXgh7+nAn7NZV5wGW0y5Xl8JUq2jZcB3wzMRvgpsj/vY3ObMbCJwDTDZObcrrr2fmeWFw0cAQ4F3slhXqtduGXC+mXUzs8KwrleyVVecCcAG51x1rCFb6yxVPtDW77G2Ptqb6RvB0eA3Cf6zzs1hHV8j+Li0BqgIb2cCC4G1YfsyoH+W6zqC4AyD1cC62DoC+gBPAW8BK4DeOVhnBwM1QK+4tpysL4J/KluAWoL9ld9PtY4Izjy4K3zPrQWKslxXJcH+1dj77Ddh3/8IX+MKYBXwrSzXlfK1A+aG62sjMCnbr2XY/gAwK6FvVtZZI/nQpu8xffVfRKSD8G2Xi4iIpKBAFxHpIBToIiIdhAJdRKSDUKCLiHQQCnQRkQ5CgS4i0kH8f1Bf+/K7KfLpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEICAYAAAB25L6yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3RU9b338fc3JBC5CALRIhGCp4JVgQSCqAheexSloFSrnCikHEU4PlWx1dLSFtoenvWcU9plWafaohZpjaLVlnqjWlSK1GtAqqJwFAWNAsZYIQjI7fv8sXeSyX2SzGUTPq+1Zs3Mnn35zp7JJ7/57Zu5OyIiEl0Z6S5ARESapqAWEYk4BbWISMQpqEVEIk5BLSIScQpqEZGIU1AfZsxsmZlNSfS46WRmm8zs/CTM183sy+HjX5vZD+MZtxXLKTKzp1pbZxPzPdvMyhI9X0m9zHQXIM0zs50xTzsDXwAHwufXuXtJvPNy97HJGLe9c/fpiZiPmeUB7wFZ7r4/nHcJEPdnKIcfBfUhwN27Vj02s03ANe6+vO54ZpZZ9ccvIu2Huj4OYVU/bc3su2a2FVhkZkeZ2WNmVm5m/wwf58ZMs8LMrgkfF5vZKjObH477npmNbeW4A8xspZlVmtlyM/uVmd3bSN3x1PhTM/t7OL+nzKx3zOtXm9lmM6sws9lNrJ+RZrbVzDrEDLvUzF4LH59qZi+Y2WdmtsXM/sfMOjYyr3vM7D9jnt8STvORmU2tM+7FZvaqme0wsw/MbG7MyyvD+8/MbKeZnV61bmOmP8PMXjGz7eH9GfGum6aY2VfC6T8zs3VmNj7mtYvM7M1wnh+a2XfC4b3Dz+czM/vUzJ4zM+VGimmFH/q+BPQE+gPTCD7TReHzfsBu4H+amH4ksAHoDfw3cLeZWSvGvQ94GegFzAWubmKZ8dT4b8A3gaOBjkBVcJwE3BHO/9hwebk0wN1fAj4Hzq0z3/vCxweAmeH7OR04D/iPJuomrOHCsJ6vAicAdfvHPwcmAz2Ai4EZZnZJ+NqY8L6Hu3d19xfqzLsn8DiwIHxvvwAeN7Nedd5DvXXTTM1ZwKPAU+F03wJKzGxQOMrdBN1o3YBTgGfC4d8GyoAc4Bjg+4DOO5FiCupD30Fgjrt/4e673b3C3R92913uXgnMA85qYvrN7n6nux8AFgN9CP4g4x7XzPoBI4Afufted18FPNLYAuOscZG7/6+77wYeBPLD4ZcBj7n7Snf/AvhhuA4acz8wCcDMugEXhcNw99Xu/qK773f3TcBvGqijId8I63vD3T8n+McU+/5WuPvr7n7Q3V8LlxfPfCEI9rfd/fdhXfcD64GvxYzT2LppymlAV+D/hZ/RM8BjhOsG2AecZGZHuvs/3X1NzPA+QH933+fuz7lOEJRyCupDX7m776l6Ymadzew3YdfADoKf2j1if/7XsbXqgbvvCh92beG4xwKfxgwD+KCxguOscWvM410xNR0bO+8wKCsaWxZB63mimXUCJgJr3H1zWMfA8Gf91rCO/0vQum5OrRqAzXXe30gzezbs2tkOTI9zvlXz3lxn2Gagb8zzxtZNszW7e+w/tdj5fp3gn9hmM/ubmZ0eDv8Z8A7wlJm9a2az4nsbkkgK6kNf3dbNt4FBwEh3P5Kan9qNdWckwhagp5l1jhl2XBPjt6XGLbHzDpfZq7GR3f1NgkAaS+1uDwi6UNYDJ4R1fL81NRB038S6j+AXxXHu3h34dcx8m2uNfkTQJRSrH/BhHHU1N9/j6vQvV8/X3V9x9wkE3SJLCVrquHulu3/b3Y8HxgM3m9l5baxFWkhB3f50I+jz/Szs75yT7AWGLdRSYK6ZdQxbY19rYpK21PgQMM7Mzgw3/P2E5r/H9wE3EvxD+EOdOnYAO83sRGBGnDU8CBSb2UnhP4q69Xcj+IWxx8xOJfgHUaWcoKvm+Ebm/QQw0Mz+zcwyzewK4CSCboq2eImg9X2rmWWZ2dkEn9GS8DMrMrPu7r6PYJ0cBDCzcWb25XBbxHaCfv2mupokCRTU7c9twBHAJ8CLwF9StNwigg1yFcB/Ag8Q7O/dkFbX6O7rgOsJwncL8E+CjV1NqeojfsbdP4kZ/h2CEK0E7gxrjqeGZeF7eIagW+CZOqP8B/ATM6sEfkTYOg2n3UXQJ//3cE+K0+rMuwIYR/CrowK4FRhXp+4Wc/e9BME8lmC93w5Mdvf14ShXA5vCLqDpBJ8nBBtLlwM7gReA29392bbUIi1n2i4gyWBmDwDr3T3pLXqR9k4takkIMxthZv9iZhnh7msTCPo6RaSNdGSiJMqXgD8SbNgrA2a4+6vpLUmkfVDXh4hIxKnrQ0Qk4pLS9dG7d2/Py8tLxqxFRNql1atXf+LuOQ29lpSgzsvLo7S0NBmzFhFpl8ys7hGp1dT1ISIScQpqEZGIU1CLiESc9qMWaQf27dtHWVkZe/bsaX5kSavs7Gxyc3PJysqKexoFtUg7UFZWRrdu3cjLy6Px6z5Iurk7FRUVlJWVMWDAgLini0zXR0kJ5OVBRkZwX6JLfYrEbc+ePfTq1UshHXFmRq9evVr8yycSLeqSEpg2DXaFp53fvDl4DlBU1Ph0IlJDIX1oaM3nFIkW9ezZNSFdZdeuYLiIyOEuEkH9/vstGy4i0VJRUUF+fj75+fl86Utfom/fvtXP9+7d2+S0paWl3HDDDc0u44wzzmh2nHisWLGCcePGJWReqRKJoO5X90JGzQwXkbZJ9DahXr16sXbtWtauXcv06dOZOXNm9fOOHTuyf//+RqctLCxkwYIFzS7j+eefb1uRh7BIBPW8edC5c+1hnTsHw0Uksaq2CW3eDO4124QSvQG/uLiY6dOnM3LkSG699VZefvllTj/9dAoKCjjjjDPYsGEDULuFO3fuXKZOncrZZ5/N8ccfXyvAu3btWj3+2WefzWWXXcaJJ55IUVERVWcBfeKJJzjxxBMZPnw4N9xwQ7Mt508//ZRLLrmEIUOGcNppp/Haa68B8Le//a36F0FBQQGVlZVs2bKFMWPGkJ+fzymnnMJzzz2X2BXWhEhsTKzaYDh7dtDd0a9fENLakCiSeE1tE0r031xZWRnPP/88HTp0YMeOHTz33HNkZmayfPlyvv/97/Pwww/Xm2b9+vU8++yzVFZWMmjQIGbMmFFvn+NXX32VdevWceyxxzJq1Cj+/ve/U1hYyHXXXcfKlSsZMGAAkyZNara+OXPmUFBQwNKlS3nmmWeYPHkya9euZf78+fzqV79i1KhR7Ny5k+zsbBYuXMgFF1zA7NmzOXDgALvqrsQkikRQQ/AFUTCLJF8qtwldfvnldOjQAYDt27czZcoU3n77bcyMffv2NTjNxRdfTKdOnejUqRNHH30027ZtIzc3t9Y4p556avWw/Px8Nm3aRNeuXTn++OOr90+eNGkSCxcubLK+VatWVf+zOPfcc6moqGDHjh2MGjWKm2++maKiIiZOnEhubi4jRoxg6tSp7Nu3j0suuYT8/Pw2rZuWiETXh4ikTiq3CXXp0qX68Q9/+EPOOecc3njjDR599NFG9yXu1KlT9eMOHTo02L8dzzhtMWvWLO666y52797NqFGjWL9+PWPGjGHlypX07duX4uJifve73yV0mU1RUIscZtK1TWj79u307dsXgHvuuSfh8x80aBDvvvsumzZtAuCBB5q/qPzo0aMpCTvnV6xYQe/evTnyyCPZuHEjgwcP5rvf/S4jRoxg/fr1bN68mWOOOYZrr72Wa665hjVr1iT8PTRGQS1ymCkqgoULoX9/MAvuFy5Mftfjrbfeyve+9z0KCgoS3gIGOOKII7j99tu58MILGT58ON26daN79+5NTjN37lxWr17NkCFDmDVrFosXLwbgtttu45RTTmHIkCFkZWUxduxYVqxYwdChQykoKOCBBx7gxhtvTPh7aExSrplYWFjounCASOq89dZbfOUrX0l3GWm3c+dOunbtirtz/fXXc8IJJzBz5sx0l1VPQ5+Xma1298KGxleLWkTajTvvvJP8/HxOPvlktm/fznXXXZfukhIirr0+zGwmcA3gwOvAN91d51MUkUiZOXNmJFvQbdVsi9rM+gI3AIXufgrQAbgy2YWJiEgg3q6PTOAIM8sEOgMfJa8kERGJ1WxQu/uHwHzgfWALsN3dn6o7nplNM7NSMystLy9PfKUiIoepeLo+jgImAAOAY4EuZnZV3fHcfaG7F7p7YU5OTuIrFRE5TMXT9XE+8J67l7v7PuCPQGLONygi7cI555zDk08+WWvYbbfdxowZMxqd5uyzz6ZqN96LLrqIzz77rN44c+fOZf78+U0ue+nSpbz55pvVz3/0ox+xfPnylpTfoCidDjWeoH4fOM3MOltwaYLzgLeSW5aIHEomTZrEkiVLag1bsmRJXCdGguCsdz169GjVsusG9U9+8hPOP//8Vs0rquLpo34JeAhYQ7BrXgbQ9JlOROSwctlll/H4449XXyRg06ZNfPTRR4wePZoZM2ZQWFjIySefzJw5cxqcPi8vj08++QSAefPmMXDgQM4888zqU6FCsI/0iBEjGDp0KF//+tfZtWsXzz//PI888gi33HIL+fn5bNy4keLiYh566CEAnn76aQoKChg8eDBTp07liy++qF7enDlzGDZsGIMHD2b9+vVNvr90nw41rv2o3X0O0PAaFpFIuekmWLs2sfPMz4fbbmv89Z49e3LqqaeybNkyJkyYwJIlS/jGN76BmTFv3jx69uzJgQMHOO+883jttdcYMmRIg/NZvXo1S5YsYe3atezfv59hw4YxfPhwACZOnMi1114LwA9+8APuvvtuvvWtbzF+/HjGjRvHZZddVmtee/bsobi4mKeffpqBAwcyefJk7rjjDm666SYAevfuzZo1a7j99tuZP38+d911V6PvL92nQ9WRiSKSELHdH7HdHg8++CDDhg2joKCAdevW1eqmqOu5557j0ksvpXPnzhx55JGMHz+++rU33niD0aNHM3jwYEpKSli3bl2T9WzYsIEBAwYwcOBAAKZMmcLKlSurX584cSIAw4cPrz6RU2NWrVrF1VdfDTR8OtQFCxbw2WefkZmZyYgRI1i0aBFz587l9ddfp1u3bk3OOx6ROR+1iCRGUy3fZJowYQIzZ85kzZo17Nq1i+HDh/Pee+8xf/58XnnlFY466iiKi4sbPb1pc4qLi1m6dClDhw7lnnvuYcWKFW2qt+pUqW05TeqsWbO4+OKLeeKJJxg1ahRPPvlk9elQH3/8cYqLi7n55puZPHlym2pVi1pEEqJr166cc845TJ06tbo1vWPHDrp06UL37t3Ztm0by5Yta3IeY8aMYenSpezevZvKykoeffTR6tcqKyvp06cP+/btqz41KUC3bt2orKysN69BgwaxadMm3nnnHQB+//vfc9ZZZ7XqvaX7dKhqUYtIwkyaNIlLL720uguk6rSgJ554IscddxyjRo1qcvphw4ZxxRVXMHToUI4++mhGjBhR/dpPf/pTRo4cSU5ODiNHjqwO5yuvvJJrr72WBQsWVG9EBMjOzmbRokVcfvnl7N+/nxEjRjB9+vRWva+qazkOGTKEzp071zod6rPPPktGRgYnn3wyY8eOZcmSJfzsZz8jKyuLrl27JuQCAzrNqUg7oNOcHlp0mlMRkXZGQS0iEnEKapF2IhndmJJ4rfmcFNQi7UB2djYVFRUK64hzdyoqKsjOzm7RdNrrQ6QdyM3NpaysDJ1iOPqys7PJzc1t0TQKapF2ICsriwEDBqS7DEkSdX2IiEScglpEJOIU1CIiEaegFhGJOAW1iEjEKahFRCJOQS0iEnHNBrWZDTKztTG3HWZ2UyqKExGROA54cfcNQD6AmXUAPgT+lOS6REQk1NKuj/OAje6+ORnFiIhIfS0N6iuB+xt6wcymmVmpmZXqfAMiIokTd1CbWUdgPPCHhl5394XuXujuhTk5OYmqT0TksNeSFvVYYI27b0tWMSIiUl9LgnoSjXR7iIhI8sQV1GbWBfgq8MfkliMiInXFdT5qd/8c6JXkWkREpAE6MlFEJOIU1CIiEaegFhGJOAW1iEjEKahFRCJOQS0iEnEKahGRiFNQi4hEnIJaRCTiFNQiIhGnoBYRiTgFtYhIxCmoRUQiTkEtIhJxCmoRkYhTUIuIRJyCWkQk4hTUIiIRp6AWEYm4eC9u28PMHjKz9Wb2lpmdnuzCREQkENfFbYFfAn9x98vMrCPQOYk1iYhIjGaD2sy6A2OAYgB33wvsTW5ZIiJSJZ6ujwFAObDIzF41s7vMrEvdkcxsmpmVmllpeXl5wgsVETlcxRPUmcAw4A53LwA+B2bVHcndF7p7obsX5uTkJLhMEZHDVzxBXQaUuftL4fOHCIJbRERSoNmgdvetwAdmNigcdB7wZlKrEhGRavHu9fEtoCTc4+Nd4JvJK0lERGLFFdTuvhYoTHItIiLSAB2ZKCIScZEK6g0bYNu2dFchIhItkQrqggL4+c/TXYWISLREKqizsmDfvnRXISISLQpqEZGIi1xQ79+f7ipERKIlUkGdmakWtYhIXZEKanV9iIjUp6AWEYk4BbWISMRFKqgzM7UxUUSkrkgFtVrUIiL1KahFRCJOQS0iEnGRC2r1UYuI1BapoNYBLyIi9UUqqNX1ISJSn4JaRCTiFNQiIhEX1zUTzWwTUAkcAPa7e1Kun6gDXkRE6ov3KuQA57j7J0mrBLWoRUQaoq4PEZGIizeoHXjKzFab2bSGRjCzaWZWamal5eXlrSpGQS0iUl+8QX2muw8DxgLXm9mYuiO4+0J3L3T3wpycnFYVo6AWEakvrqB29w/D+4+BPwGnJqOYt9+G7dshIwPy8qCkJBlLERE5tDQb1GbWxcy6VT0G/hV4I9GFlJTAX/8K7sFt82aYNk1hLSIST4v6GGCVmf0DeBl43N3/kuhCZs+uv2verl3BcBGRw1mzu+e5+7vA0GQX8v77LRsuInK4iMzuef36tWy4iMjhIjJBPW9esNdHrM6dg+EiIoezyAR1URFccUXN8/79YeHCYLiIyOGsJYeQJ91pp8G998LHH0Mrd8UWEWl3ItOihpquDx30IiJSI1JBnRm273UGPRGRGpEKarWoRUTqU1CLiEScglpEJOIiFdTqoxYRqS9SQa0WtYhIfQpqEZGIU1CLiEScglpEJOIiFdTamCgiUl+kglotahGR+hTUIiIRp6AWEYm4uIPazDqY2atm9liyiqnqo1ZQi4jUaEmL+kbgrWQVAjUtam1MFBGpEVdQm1kucDFwVzKLUdeHiEh98baobwNuBQ4msRYFtYhIA5oNajMbB3zs7qubGW+amZWaWWl5eXmrilFQi4jUF0+LehQw3sw2AUuAc83s3rojuftCdy9098KcVl7wUAe8iIjU12xQu/v33D3X3fOAK4Fn3P2qZBSjFrWISH3aj1pEJOIyWzKyu68AViSlEiAjI7gpqEVEakSqRQ1BP7WCWkSkRuSCOitLGxNFRGJFMqjVohYRqaGgFhGJOAW1iEjERS6oMzPVRy0iEityQa0WtYhIbQpqEZGIU1CLiERc5IJaB7yIiNQWuaDWAS8iIrVFMqjVohYRqaGgFhGJOAW1iEjERS6odcCLiEhtkQtqtahFRGpTUIuIRJyCWkQk4iIX1DrgRUSktmaD2syyzexlM/uHma0zsx8nsyAd8CIiUls8F7f9AjjX3XeaWRawysyWufuLyShIXR8iIrU1G9Tu7sDO8GlWePNkFaSgFhGpLa4+ajPrYGZrgY+Bv7r7Sw2MM83MSs2stLy8vNUFKahFRGqLK6jd/YC75wO5wKlmdkoD4yx090J3L8zJyWl1QdqYKCJSW4v2+nD3z4BngQuTU442JoqI1BXPXh85ZtYjfHwE8FVgfbIKqur68KT1gouIHFri2eujD7DYzDoQBPuD7v5YsgrKygruDxwIukFERA538ez18RpQkIJagJpw3rdPQS0iAhE8MrGqRa1+ahGRQOSCOjs7uN+9O711iIhEReSC+phjgvtt29Jbh4hIVEQuqI89Nrj/6KP01iEiEhUKahGRiItcUPfpE9x/+GF66xARiYrIBXWnTtCrl1rUIiJVIhfUEHR/KKhFRAIKahGRiFNQi4hEXGSDeuvW4HwfIiKHu0gGdVlZzUmZ8vKgpCTdFYmIpE/kgrqkBO6/v+b55s0wbZrCWkQOX5EL6tmzYe/e2sN27QqGi4gcjiIX1O+/37LhIiLtXeSCul+/hod36QIXXwyrVqW2HhGRdItcUM+bB5071x9+8CCsWQNjxsDVV8Of/wyvvw6ff576GkVEUilyQV1UBAsX1pycqXNnuPlm2LED3n4bbrwRHn0ULrkEhgyBo4+GyZNh48b01i0ikizmSbiKbGFhoZeWliZ8vlX27oWXXgoOinnmGbjvvmB3vgUL4JprkrZYEZGkMbPV7l7Y0GvxXIX8ODN71szeNLN1ZnZj4ktsmY4dYfRouOIK+M1v4K23YNQouO46ePnldFcnIpJY8XR97Ae+7e4nAacB15vZScktq2Vyc+Ghh4LukilTYM+edFckIpI4zQa1u29x9zXh40rgLaBvsgtrqe7d4a67YP364F5EpL1o0cZEM8sDCoCXGnhtmpmVmllpeXl5YqoLlZQEh5JnZDR9SPkFF0BBAdxzT0IXLyKSVnEHtZl1BR4GbnL3HXVfd/eF7l7o7oU5OTltLqwqnM3gqquCQ8ndg/urroLevRsO7ClTYPVqWLeuzSWIiERCXEFtZlkEIV3i7n9MbklBAE+bFoRyYyoqgsA2q93KnjQpOJnT4sXJrlJEJDWa3T3PzAxYDHzq7jfFM9O27p6Xl9d0SDelV6/g6MatW4PDzjMzW12GiEjKtGn3PGAUcDVwrpmtDW8XJbTCOtpyXo+KiuCIxS1bYPnyxNUkIpIuzbY33X0VYCmopVq/fq1vUQPs3x/c//jHcOGFialJRCRdIncIOTR+vo+WevFFuPPOts9HRCSdIhnUVef76N8/eN6hQ3DfpUvL53VTXL3qIiLRFcmghiCsN20Kdsnbvz+437kT7r23JsAtjg6ZXbt0AIyIHNoiG9SNiQ3wgweD4O7Vq+lpvvOdlJQmIpIUh1xQ11VUBJ98EgR2Y7ZvD7pSREQORYd8UFcpKmq6ZX3ddXDLLUFLXETkUNKuDgf55S+DIxp37ar/WmYmzJ8f7Pb35S8H3SYHDgT3PXsG/d79+8PgwdCjR+prFxFpTLsK6qKi4H7KlCCEY+3fD9nZsHRp0KrOyAj2JsnIqH05L7PgxE4TJ8Lw4cG4Td2qpsnICO7NYPfu4MCbzZuhsjK40MHevfDFFzWPY2uounXpAl27NnzLzo5v42lT2jp9ouYRu/4OHKh9O3iw8eU19rhqng3dQ+11XPWZx6sl71fjJnfcREvGr+uOHeGssxI/30PyCi/Nycho/EO4996aQK+ye3dwNOR77wUXHvjLX+CFF9peR2YmHHkkdOoUfIBVt6ysoMbYcNq3L/glsHNncKv7j0ZEou+YY4LTV7RGU4eQt8ugbu5cIb16Bd0kdQM7VlkZfPBBTSu5sRvUtA4PHgzujzgCjjoK+vSp2Qe8JdyD1ndVaO/c2faLIbjDsmXwi18EG1eTxSw12wG0HC0nqsuJJ18aXsZhFtQlJcGZ9doiIyMI3mR/EVK1HBFJnY4d4be/bVlYt/WkTIec5vYAiUdVX2mywzNVyxGR1Nm7F2bPTtz82mVQQ/DTIxHnCxERaY22nAW0rna110espvYAERFJtn79EjevdtuihiCsFy8O9rIQEUmVjh2Ds4AmSrsOagjCetGitvdZS3yq9lFO9v6xWo6WE9Xl9OrV8g2JzWm3XR+xiopqVlpJCdx4Y3BASlPa214fVcvp3z/4T5/IL5GIJNdhEdSxYkNbRORQ0O67PkREDnXNBrWZ/dbMPjazN1JRkIiI1BZPi/oeQJeIFRFJk2aD2t1XAp+moBYREWlAwvqozWyamZWaWWl5eXmiZisictiL66RMZpYHPObup8Q1U7NyoInz1zWpN/BJK6dNJtXVclGtTXW1jOpqudbU1t/dcxp6ISm75zW2sHiYWWljZ5BKJ9XVclGtTXW1jOpquUTXpt3zREQiLp7d8+4HXgAGmVmZmf178ssSEZEqzXZ9uPukVBQSY2GKlxcv1dVyUa1NdbWM6mq5hNaWlCu8iIhI4qiPWkQk4hTUIiIRF5mgNrMLzWyDmb1jZrPSWMdxZvasmb1pZuvM7MZw+Fwz+9DM1oa3i9JU3yYzez2soTQc1tPM/mpmb4f3R6W4pkEx62Wtme0ws5vSsc4aOjdNY+vHAgvC79xrZjYsDbX9zMzWh8v/k5n1CIfnmdnumHX36xTX1ehnZ2bfC9fZBjO7IMV1PRBT0yYzWxsOT+X6aiwjkvc9c/e034AOwEbgeKAj8A/gpDTV0gcYFj7uBvwvcBIwF/hOBNbVJqB3nWH/DcwKH88C/ivNn+VWoH861hkwBhgGvNHc+gEuApYBBpwGvJSG2v4VyAwf/1dMbXmx46WhrgY/u/Bv4R9AJ2BA+HfbIVV11Xn958CP0rC+GsuIpH3PotKiPhV4x93fdfe9wBJgQjoKcfct7r4mfFwJvAX0TUctLTABWBw+XgxcksZazgM2untrj0xtE2/43DSNrZ8JwO888CLQw8z6pLI2d3/K3feHT18EcpO1/JbU1YQJwBJ3/8Ld3wPeIfj7TWldZmbAN4D7k7HspjSREUn7nkUlqPsCH8Q8LyMC4RgeOl8AvBQO+j/hT5ffprp7IYYDT5nZajObFg47xt23hI+3AsekpzQArqT2H08U1llj6ydq37upBC2vKgPM7FUz+5uZjU5DPQ19dlFZZ6OBbe7+dsywlK+vOhmRtO9ZVII6csysK/AwcJO77wDuAP4FyAe2EPzsSocz3X0YMBa43szGxL7owW+ttOxzaWYdgfHAH8JBUVln1dK5fppiZrOB/UBJOGgL0M/dC4CbgfvM7MgUlhS5z66OSdRuEKR8fTWQEdUS/T2LSlB/CBwX8zw3HJYWZpZF8AGUuPsfAdx9m7sfcPeDwJ0k6edec9z9w/D+Y+BPYR3bqjL6j+MAAAGiSURBVH5Khfcfp6M2gn8ea9x9W1hjJNYZja+fSHzvzKwYGAcUhX/ghF0LFeHj1QR9wQNTVVMTn13a15mZZQITgQeqhqV6fTWUESTxexaVoH4FOMHMBoStsiuBR9JRSNj3dTfwlrv/ImZ4bJ/SpUDKr3hjZl3MrFvVY4INUW8QrKsp4WhTgD+nurZQrVZOFNZZqLH18wgwOdwqfxqwPeana0qY2YXArcB4d98VMzzHzDqEj48HTgDeTWFdjX12jwBXmlknMxsQ1vVyquoKnQ+sd/eyqgGpXF+NZQTJ/J6lYitpnFtSLyLYeroRmJ3GOs4k+MnyGrA2vF0E/B54PRz+CNAnDbUdT7DF/R/Auqr1BPQCngbeBpYDPdNQWxegAugeMyzl64zgH8UWYB9BX+C/N7Z+CLbC/yr8zr0OFKahtncI+i+rvmu/Dsf9evgZrwXWAF9LcV2NfnbA7HCdbQDGprKucPg9wPQ646ZyfTWWEUn7nukQchGRiItK14eIiDRCQS0iEnEKahGRiFNQi4hEnIJaRCTiFNQiIhGnoBYRibj/D2h6nkKXId/oAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_44LGpK-xx4"
      },
      "source": [
        "def final_predictions(text):\n",
        "  y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
        "  y_id_to_word[0] = '<PAD>'\n",
        "\n",
        "  sentence = [english_tokenizer.word_index[word] for word in text.split()]\n",
        "  sentence = pad_sequences([sentence], maxlen=preproc_french_sentences.shape[-2], padding='post')\n",
        "  \n",
        "  print(sentence.shape)\n",
        "  print(logits_to_text(simple_rnn_model.predict(sentence[:1])[0], french_tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm-DAR9H-4xD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "e59e5f98-06d7-4b9b-a28b-2ba44725328e"
      },
      "source": [
        "DNN_Model.save('model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-5cdcfae8caba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimple_rnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'simple_rnn_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onnTqdkf-_ry"
      },
      "source": [
        "! mkdir \"pickles\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhDpg-NB_AXV"
      },
      "source": [
        "with open('pickles/french_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(french_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('pickles/english_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(english_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('pickles/preproc_french_sentences.pickle', 'wb') as handle:\n",
        "    pickle.dump(preproc_french_sentences, handle, protocol=pickle.HIGHEST_PROTOCOL) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}