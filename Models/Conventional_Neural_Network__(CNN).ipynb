{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conventional_Neural_Network _(CNN).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyCAqBpNCnPq"
      },
      "source": [
        "%%capture\n",
        "# Install the latest Tensorflow version.\n",
        "!pip install tensorflow_text\n",
        "!pip install bokeh\n",
        "!pip install simpleneighbors[annoy]\n",
        "!pip install tqdm\n",
        "!pip install unidecode"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMNG_uy7DoDD"
      },
      "source": [
        "import os,re,collections\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GRU,LSTM, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "import tensorflow.compat.v2 as tf\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vt2rQCOPz74"
      },
      "source": [
        "## ***Etape 1 : Chargement de données*** \n",
        " Nous allons extraire les données qui se trouvent dans le zip dans le drive ainsi pouvoirs les exploité facilement puis nous allons organiser les fichier de sort a ce que chaque phrase aille sa traduction dans le fichier correspondant,les phrase qui seront extraitent du fichier vont étre prétraiter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc-Nd6AxZzyd"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "#nous allons extraire nos données qui se trouvent dans un fichier zip dans notre drive \n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/rebuilt.Cross-Language-Dataset-master.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NohB7jO3jL0w"
      },
      "source": [
        "#nous allons experimenter une partie de nos donnée dans cette partie on commence par le corpus APR \n",
        "English_Dataset= tf.data.Dataset.list_files(\"/tmp/Cross-Language-Dataset-master/dataset/chunks/APR/en/*.txt\")\n",
        "Frensh_Dataset = tf.data.Dataset.list_files(\"/tmp/Cross-Language-Dataset-master/dataset/chunks/APR/fr/*.txt\")\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH58fu-j_in7"
      },
      "source": [
        "#nous allons organiser nos fichier text de sort a ce que les fichier en francais aille leur traduction anglaise\n",
        "English_DatasetFiles=list(English_Dataset.as_numpy_iterator())\n",
        "English_DatasetFiles.sort()\n",
        "Frensh_DatasetFiles=list(Frensh_Dataset .as_numpy_iterator())\n",
        "Frensh_DatasetFiles.sort()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtJR_FFUUQ0d"
      },
      "source": [
        "#cette fonction permet d'extraire extrait les donnees des fichier\n",
        "def load_data(Files):\n",
        "  sentences=[]\n",
        "  for file_content in Files: \n",
        "    with open(file_content, \"r\") as f:\n",
        "      data = f.read()\n",
        "      sentences.append(data)\n",
        "  return sentences\n",
        "\n",
        "english_sentences=load_data(English_DatasetFiles)\n",
        "french_sentences=load_data(Frensh_DatasetFiles)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITcf0jAdfy6u",
        "outputId": "f5dd275b-6f19-47af-d5ae-264b75d2f58c"
      },
      "source": [
        "for i in range(5):\n",
        "  print('Sample :',i)\n",
        "  print(english_sentences[i])\n",
        "  print(french_sentences[i])\n",
        "  print('-'*50)\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample : 0\n",
            "the association gregory lemarchal\n",
            "\n",
            "l' association gregory lemarchal\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 1\n",
            "after the dresden file\n",
            "\n",
            "après les dossier dresden\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 2\n",
            "the tradition of lanfeust troy\n",
            "invention of reverse auctions\n",
            "\n",
            "la tradition lanfeust de troy\n",
            "invention des ventes aux enchères\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 3\n",
            "the manner of dos passos\n",
            "\n",
            "la manière de dos passos\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 4\n",
            "trace of fat in this book\n",
            "\n",
            "trace de gras dans le livre\n",
            "\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcTWq6nvOmDQ"
      },
      "source": [
        "# ***Etape 2 : Stopwords***\n",
        "première manipulation souvent effectuée dans le traitement du texte est la suppression de ce qu'on appelle en anglais les stopwords. Ce sont les mots très courants dans la langue étudiée (\"et\", \"à\", \"le\"... en français) qui n'apportent pas de valeur informative pour la compréhension du \"sens\" d'un document et corpus. Il sont très fréquents et ralentissent notre travail : nous souhaitons donc les supprimer.\n",
        "\n",
        "Il existe dans la librairie NLTK une liste par défaut des stopwords dans plusieurs Langues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0hrikyVlnAw",
        "outputId": "79a6e40a-3580-4d46-fceb-48f499c26b75"
      },
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#pour supprimer les stopword de chaque langue\n",
        "def remove_stopwords(text,langue):\n",
        "    STOPWORDS = set(stopwords.words(langue))\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "#supprimer les stopword dans une liste de phrase\n",
        "def remove(sentences,langue):\n",
        "    values=[]\n",
        "    for text in sentences:\n",
        "       values.append(remove_stopwords(text,langue))\n",
        "    return values\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FFU2kwwlp3r"
      },
      "source": [
        "\n",
        "english_sentences=remove(english_sentences,'english')\n",
        "french_sentences=remove(french_sentences,'french')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gth9wlQr9j3f",
        "outputId": "9a236d5d-02b9-4ece-fd44-1a99ea4b0239"
      },
      "source": [
        "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
        "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
        "\n",
        "print('English Vocab:',len(english_words_counter))\n",
        "print('French Vocab:',len(french_words_counter))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocab: 3981\n",
            "French Vocab: 4271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGEo2THGR3hr"
      },
      "source": [
        "# ***Etape 3 :le Stemming*** \n",
        "racinisation(ou stemming en anglais). Cela consiste à ne conserver que la racine des mots étudiés. L'idée étant de supprimer les suffixes, préfixes et autres des mots afin de ne conserver que leur origine. C'est un procédé plus simple que la lemmatisation et plus rapide à effectuer puisqu'on tronque les mots essentiellement contrairement à la lemmatisation qui nécessite d'utiliser un dictionnaire.Dans notre cas, on va effectuer une racinisation parce qu'il n'existe pas de fonction de lemmatisation de corpus français dans NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3gDnA9XoD2Q"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "def stem_words_langue(text,langue):\n",
        "    stemmer2=SnowballStemmer(langue)\n",
        "    return \" \".join([stemmer2.stem(word) for word in text.split()])\n",
        "\n",
        "def stemming_langue(sentences,langue):\n",
        "  values=[]\n",
        "  for text in sentences:\n",
        "    values.append(stem_words_langue(text,langue))\n",
        "  return values\n",
        " "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zds9TdSQEnn"
      },
      "source": [
        "\n",
        "english_sentences=stemming_langue(english_sentences,'english')\n",
        "french_sentences=stemming_langue(french_sentences,'french')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfDrPIYqTiPv"
      },
      "source": [
        "## ***Etape 4 : la tokenisation***\n",
        "La tokenisation est la division du texte brut en petits morceaux de mots ou de phrases, appelés jetons.Chaque phrase prend son sens par les mots qu'elle contient. Ainsi, en analysant les mots présents dans le texte, nous pouvons facilement interpréter le sens du texte. Une fois que nous avons une liste de mots, nous pouvons également utiliser des outils et des méthodes statistiques pour mieux comprendre le texte. Par exemple, nous pouvons utiliser le nombre de mots et la fréquence des mots pour découvrir l'importance d'un mot dans cette phrase ou ce document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP0AOadJ9lCz"
      },
      "source": [
        "def tokenize(x):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(x)\n",
        "  return tokenizer.texts_to_sequences(x), tokenizer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL_rK09M9vJf"
      },
      "source": [
        "def pad(x, length=None):\n",
        "  return pad_sequences(x, maxlen=length, padding='post')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7_d5he_9ywE"
      },
      "source": [
        "def preprocess(x, y):\n",
        "    \"\"\"\n",
        "      Prétraiter x et y\n",
        "    : param x: Feature Liste des phrases\n",
        "    : param y: Label Liste des phrases\n",
        "    : return: Tuple de (pré-traité x, pré-traité y, x tokenizer, y tokenizer)\n",
        "    \"\"\"\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # La fonction sparse_categorical_crossentropy de Keras nécessite que les étiquettes soient en 3 dimensions\n",
        "    # Extension des dimensions\n",
        "    \n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yDJo4gD96lH"
      },
      "source": [
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
        "    preprocess(english_sentences, french_sentences)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5tS4iyl-Lhr",
        "outputId": "4eea205d-a197-4455-9640-47eef008b630"
      },
      "source": [
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Preprocessed\n",
            "Max English sentence length: 31\n",
            "Max French sentence length: 34\n",
            "English vocabulary size: 3616\n",
            "French vocabulary size: 3766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxd8wRHV-OlA"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "  index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "  index_to_words[0] = '<PAD>'\n",
        "\n",
        "   # Nous prédisons la sortie pour un mot donné, puis sélectionnons la meilleure réponse\n",
        "   # En sélectionnant cette étiquette, nous énumérons le mot à partir de l'identifiant\n",
        "    \n",
        "  return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkGroinK-g_9"
      },
      "source": [
        "\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzq41ASNU-pX"
      },
      "source": [
        "# ***Etape 5 les Word embedding*** \n",
        "Nous allons utilisée un prétrained model Glove word embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCwMiNIc56t4",
        "outputId": "cc136381-8f24-4f6d-fff0-de33762162de"
      },
      "source": [
        "glove_dir = '/content/drive/MyDrive/glove/'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByXen8xW59C4"
      },
      "source": [
        "embedding_dim = 100\n",
        "max_words = english_vocab_size+1\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in english_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EoAYcFZWPVi"
      },
      "source": [
        "# ***Etape 6 : Model***\n",
        "Nous allons maintenant procedé a la creation de notre model neuronal ,l'entrainer et l'evaluer puis afficher les resulat donné a partir d'un graphe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUOB9AAhehFN"
      },
      "source": [
        "from keras.layers import Dense\n",
        "from keras.layers import Conv1D, MaxPooling1D,Flatten\n",
        "from keras import metrics,layers\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6hbZcofTqN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646939ee-06bf-4b79-9217-1d6f3b7f246f"
      },
      "source": [
        "from keras.layers import Dense\n",
        "from keras.layers import Conv1D, MaxPooling1D,Flatten\n",
        "from keras import metrics,layers\n",
        "\n",
        "def embed_modelCNN(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \n",
        "   \n",
        "    # Hyperparamètres\n",
        "     learning_rate = 0.05\n",
        "    \n",
        "# define model\n",
        "     model = Sequential()\n",
        "     model.add(Embedding(english_vocab_size, 100, weights=[embedding_matrix], input_shape=input_shape[1:],  trainable=False))\n",
        "     model.add(layers.Conv1D(32, 3,padding=\"same\", activation='relu'))\n",
        "     model.add(layers.MaxPooling1D(5))\n",
        "     model.add(layers.Conv1D(french_vocab_size, 3, activation='relu'))\n",
        "     model.add(layers.GlobalMaxPooling1D())\n",
        "     model.add(layers.Dense(1))\n",
        "     print(model.summary())\n",
        "    \n",
        "     model.compile(loss='mean_absolute_error', optimizer=Adam(learning_rate),metrics=['accuracy'])\n",
        "     return model\n",
        "\n",
        "cnn_model = embed_modelCNN(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "history=cnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=200, validation_split=0.2)\n",
        "scores = cnn_model.evaluate(tmp_x, preproc_french_sentences, verbose=0)\n",
        "# Displays the accuracy of correct sentiment prediction over test data\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 34, 100)           361700    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 34, 32)            9632      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 6, 32)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 4, 3767)           365399    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 3767)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 3768      \n",
            "=================================================================\n",
            "Total params: 740,499\n",
            "Trainable params: 378,799\n",
            "Non-trainable params: 361,700\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "2/2 [==============================] - 17s 943ms/step - loss: 159.3894 - accuracy: 0.6956 - val_loss: 204.2736 - val_accuracy: 0.0054\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 1s 428ms/step - loss: 136.6606 - accuracy: 0.1442 - val_loss: 194.5105 - val_accuracy: 0.8334\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 1s 446ms/step - loss: 129.6669 - accuracy: 0.8477 - val_loss: 194.4158 - val_accuracy: 0.8193\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 1s 428ms/step - loss: 128.0117 - accuracy: 0.8395 - val_loss: 194.0383 - val_accuracy: 0.8177\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 129.0410 - accuracy: 0.8343 - val_loss: 193.9906 - val_accuracy: 0.8257\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 1s 424ms/step - loss: 130.5463 - accuracy: 0.8429 - val_loss: 194.0063 - val_accuracy: 0.8334\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 1s 442ms/step - loss: 130.7681 - accuracy: 0.8473 - val_loss: 193.9907 - val_accuracy: 0.8334\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 1s 428ms/step - loss: 130.0259 - accuracy: 0.8478 - val_loss: 193.9937 - val_accuracy: 0.8302\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 1s 428ms/step - loss: 129.4780 - accuracy: 0.8484 - val_loss: 193.9996 - val_accuracy: 0.8276\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 1s 440ms/step - loss: 128.2340 - accuracy: 0.8478 - val_loss: 193.9967 - val_accuracy: 0.8245\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 128.7941 - accuracy: 0.8443 - val_loss: 194.0021 - val_accuracy: 0.8153\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 1s 440ms/step - loss: 129.5649 - accuracy: 0.8410 - val_loss: 194.0005 - val_accuracy: 0.8100\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 128.2160 - accuracy: 0.8409 - val_loss: 194.0034 - val_accuracy: 0.8077\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 1s 443ms/step - loss: 130.4293 - accuracy: 0.8378 - val_loss: 194.0085 - val_accuracy: 0.8061\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 1s 429ms/step - loss: 129.7563 - accuracy: 0.8359 - val_loss: 194.0158 - val_accuracy: 0.8078\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 129.5717 - accuracy: 0.8359 - val_loss: 194.0277 - val_accuracy: 0.8084\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 128.7520 - accuracy: 0.8365 - val_loss: 194.0370 - val_accuracy: 0.8084\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 1s 427ms/step - loss: 128.7963 - accuracy: 0.8369 - val_loss: 194.0619 - val_accuracy: 0.8084\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 129.4177 - accuracy: 0.8361 - val_loss: 194.0915 - val_accuracy: 0.8050\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 1s 431ms/step - loss: 129.7287 - accuracy: 0.8362 - val_loss: 194.1502 - val_accuracy: 0.8013\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 129.1474 - accuracy: 0.8351 - val_loss: 194.2079 - val_accuracy: 0.8052\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 1s 447ms/step - loss: 129.9394 - accuracy: 0.8373 - val_loss: 194.2765 - val_accuracy: 0.8055\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 1s 441ms/step - loss: 129.5835 - accuracy: 0.8390 - val_loss: 194.3378 - val_accuracy: 0.8074\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 1s 441ms/step - loss: 129.3425 - accuracy: 0.8401 - val_loss: 194.3951 - val_accuracy: 0.8106\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 1s 429ms/step - loss: 129.9427 - accuracy: 0.8398 - val_loss: 194.3769 - val_accuracy: 0.8156\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 129.8831 - accuracy: 0.8403 - val_loss: 194.3929 - val_accuracy: 0.8156\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 129.1976 - accuracy: 0.8424 - val_loss: 194.4070 - val_accuracy: 0.8121\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 1s 426ms/step - loss: 128.0591 - accuracy: 0.8441 - val_loss: 194.4185 - val_accuracy: 0.8177\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 128.5341 - accuracy: 0.8455 - val_loss: 194.4240 - val_accuracy: 0.8177\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 128.2266 - accuracy: 0.8457 - val_loss: 194.4756 - val_accuracy: 0.8151\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 1s 445ms/step - loss: 128.3603 - accuracy: 0.8449 - val_loss: 194.5035 - val_accuracy: 0.8140\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 128.1604 - accuracy: 0.8450 - val_loss: 194.4913 - val_accuracy: 0.8140\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 128.6555 - accuracy: 0.8443 - val_loss: 194.4995 - val_accuracy: 0.8114\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 1s 427ms/step - loss: 128.2653 - accuracy: 0.8439 - val_loss: 194.5429 - val_accuracy: 0.8140\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 1s 427ms/step - loss: 129.4062 - accuracy: 0.8452 - val_loss: 194.5652 - val_accuracy: 0.8140\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 129.8738 - accuracy: 0.8465 - val_loss: 194.6584 - val_accuracy: 0.8114\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 1s 429ms/step - loss: 128.5821 - accuracy: 0.8460 - val_loss: 194.7285 - val_accuracy: 0.8094\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 1s 441ms/step - loss: 129.2309 - accuracy: 0.8435 - val_loss: 194.6796 - val_accuracy: 0.8094\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 1s 429ms/step - loss: 128.8743 - accuracy: 0.8458 - val_loss: 194.5763 - val_accuracy: 0.8114\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 128.5032 - accuracy: 0.8467 - val_loss: 194.4857 - val_accuracy: 0.8140\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 1s 431ms/step - loss: 128.5044 - accuracy: 0.8470 - val_loss: 194.4709 - val_accuracy: 0.8166\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 1s 426ms/step - loss: 128.6815 - accuracy: 0.8466 - val_loss: 194.4956 - val_accuracy: 0.8166\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 129.1067 - accuracy: 0.8462 - val_loss: 194.5351 - val_accuracy: 0.8166\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 1s 428ms/step - loss: 129.2141 - accuracy: 0.8463 - val_loss: 194.5507 - val_accuracy: 0.8191\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 129.0205 - accuracy: 0.8459 - val_loss: 194.5350 - val_accuracy: 0.8217\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 128.5547 - accuracy: 0.8466 - val_loss: 194.4829 - val_accuracy: 0.8217\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 1s 423ms/step - loss: 129.2833 - accuracy: 0.8462 - val_loss: 194.4578 - val_accuracy: 0.8217\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 1s 448ms/step - loss: 129.1832 - accuracy: 0.8466 - val_loss: 194.4706 - val_accuracy: 0.8217\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 129.0825 - accuracy: 0.8459 - val_loss: 194.4842 - val_accuracy: 0.8217\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 128.7135 - accuracy: 0.8453 - val_loss: 194.4969 - val_accuracy: 0.8217\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 129.2184 - accuracy: 0.8458 - val_loss: 194.5112 - val_accuracy: 0.8217\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 1s 442ms/step - loss: 129.2406 - accuracy: 0.8454 - val_loss: 194.5177 - val_accuracy: 0.8217\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 1s 440ms/step - loss: 129.1539 - accuracy: 0.8447 - val_loss: 194.5231 - val_accuracy: 0.8217\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 129.1677 - accuracy: 0.8455 - val_loss: 194.5283 - val_accuracy: 0.8217\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 128.0991 - accuracy: 0.8464 - val_loss: 194.5409 - val_accuracy: 0.8217\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 128.6662 - accuracy: 0.8458 - val_loss: 194.5456 - val_accuracy: 0.8217\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 1s 441ms/step - loss: 129.8746 - accuracy: 0.8449 - val_loss: 194.5400 - val_accuracy: 0.8217\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 129.2108 - accuracy: 0.8451 - val_loss: 194.5347 - val_accuracy: 0.8217\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 128.4957 - accuracy: 0.8456 - val_loss: 194.5330 - val_accuracy: 0.8217\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 1s 442ms/step - loss: 128.6557 - accuracy: 0.8443 - val_loss: 194.5241 - val_accuracy: 0.8217\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 128.9790 - accuracy: 0.8447 - val_loss: 194.5417 - val_accuracy: 0.8217\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 129.2595 - accuracy: 0.8457 - val_loss: 194.5570 - val_accuracy: 0.8217\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 1s 426ms/step - loss: 129.0231 - accuracy: 0.8460 - val_loss: 194.5337 - val_accuracy: 0.8217\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 1s 431ms/step - loss: 128.6827 - accuracy: 0.8456 - val_loss: 194.5050 - val_accuracy: 0.8217\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 128.9597 - accuracy: 0.8454 - val_loss: 194.5141 - val_accuracy: 0.8217\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 1s 439ms/step - loss: 129.0802 - accuracy: 0.8448 - val_loss: 194.5331 - val_accuracy: 0.8217\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 1s 441ms/step - loss: 128.1988 - accuracy: 0.8462 - val_loss: 194.5311 - val_accuracy: 0.8217\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 1s 429ms/step - loss: 128.5119 - accuracy: 0.8462 - val_loss: 194.5210 - val_accuracy: 0.8217\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 129.0190 - accuracy: 0.8460 - val_loss: 194.5134 - val_accuracy: 0.8217\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 1s 424ms/step - loss: 129.7320 - accuracy: 0.8448 - val_loss: 194.5125 - val_accuracy: 0.8217\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 1s 428ms/step - loss: 129.2158 - accuracy: 0.8457 - val_loss: 194.5132 - val_accuracy: 0.8217\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 1s 440ms/step - loss: 128.7880 - accuracy: 0.8452 - val_loss: 194.5265 - val_accuracy: 0.8217\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 1s 431ms/step - loss: 129.4653 - accuracy: 0.8453 - val_loss: 194.5363 - val_accuracy: 0.8217\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 128.1702 - accuracy: 0.8461 - val_loss: 194.5466 - val_accuracy: 0.8217\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 128.2394 - accuracy: 0.8461 - val_loss: 194.5325 - val_accuracy: 0.8217\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 1s 425ms/step - loss: 128.1183 - accuracy: 0.8465 - val_loss: 194.5128 - val_accuracy: 0.8217\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 129.0847 - accuracy: 0.8455 - val_loss: 194.5063 - val_accuracy: 0.8217\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 128.7719 - accuracy: 0.8464 - val_loss: 194.5160 - val_accuracy: 0.8217\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 1s 442ms/step - loss: 129.1616 - accuracy: 0.8465 - val_loss: 194.5346 - val_accuracy: 0.8217\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 1s 431ms/step - loss: 128.9256 - accuracy: 0.8453 - val_loss: 194.5168 - val_accuracy: 0.8217\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 1s 431ms/step - loss: 128.0968 - accuracy: 0.8472 - val_loss: 194.4911 - val_accuracy: 0.8217\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 1s 426ms/step - loss: 128.4046 - accuracy: 0.8458 - val_loss: 194.4735 - val_accuracy: 0.8217\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 127.3477 - accuracy: 0.8472 - val_loss: 194.4681 - val_accuracy: 0.8217\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 1s 446ms/step - loss: 129.1557 - accuracy: 0.8461 - val_loss: 194.4828 - val_accuracy: 0.8217\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 1s 425ms/step - loss: 128.5013 - accuracy: 0.8462 - val_loss: 194.4922 - val_accuracy: 0.8217\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 129.2056 - accuracy: 0.8453 - val_loss: 194.4805 - val_accuracy: 0.8217\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 128.8186 - accuracy: 0.8466 - val_loss: 194.4871 - val_accuracy: 0.8217\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 128.8921 - accuracy: 0.8468 - val_loss: 194.4989 - val_accuracy: 0.8217\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 128.2067 - accuracy: 0.8472 - val_loss: 194.4924 - val_accuracy: 0.8217\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 128.3727 - accuracy: 0.8468 - val_loss: 194.4762 - val_accuracy: 0.8217\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 1s 439ms/step - loss: 128.6382 - accuracy: 0.8461 - val_loss: 194.4646 - val_accuracy: 0.8217\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 129.9421 - accuracy: 0.8450 - val_loss: 194.4777 - val_accuracy: 0.8217\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 128.5293 - accuracy: 0.8461 - val_loss: 194.5001 - val_accuracy: 0.8217\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 1s 424ms/step - loss: 128.9032 - accuracy: 0.8454 - val_loss: 194.5161 - val_accuracy: 0.8217\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 1s 443ms/step - loss: 129.6897 - accuracy: 0.8455 - val_loss: 194.5041 - val_accuracy: 0.8217\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 1s 440ms/step - loss: 128.0237 - accuracy: 0.8470 - val_loss: 194.4784 - val_accuracy: 0.8217\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 128.7634 - accuracy: 0.8453 - val_loss: 194.4710 - val_accuracy: 0.8217\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 130.2140 - accuracy: 0.8446 - val_loss: 194.4924 - val_accuracy: 0.8217\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 129.5725 - accuracy: 0.8458 - val_loss: 194.5098 - val_accuracy: 0.8217\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 1s 450ms/step - loss: 128.7139 - accuracy: 0.8461 - val_loss: 194.5095 - val_accuracy: 0.8217\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 127.6659 - accuracy: 0.8471 - val_loss: 194.4711 - val_accuracy: 0.8217\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 128.6888 - accuracy: 0.8463 - val_loss: 194.4372 - val_accuracy: 0.8217\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 1s 439ms/step - loss: 129.2852 - accuracy: 0.8459 - val_loss: 194.4594 - val_accuracy: 0.8217\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 1s 427ms/step - loss: 128.1082 - accuracy: 0.8472 - val_loss: 194.4832 - val_accuracy: 0.8217\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 1s 431ms/step - loss: 130.0279 - accuracy: 0.8449 - val_loss: 194.5034 - val_accuracy: 0.8191\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 1s 427ms/step - loss: 128.1053 - accuracy: 0.8469 - val_loss: 194.5022 - val_accuracy: 0.8191\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 129.9394 - accuracy: 0.8452 - val_loss: 194.4933 - val_accuracy: 0.8191\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 128.9322 - accuracy: 0.8461 - val_loss: 194.5120 - val_accuracy: 0.8217\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 128.8942 - accuracy: 0.8456 - val_loss: 194.5061 - val_accuracy: 0.8217\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 129.2729 - accuracy: 0.8450 - val_loss: 194.4880 - val_accuracy: 0.8217\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 1s 429ms/step - loss: 129.1162 - accuracy: 0.8459 - val_loss: 194.4577 - val_accuracy: 0.8217\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 1s 446ms/step - loss: 130.1958 - accuracy: 0.8454 - val_loss: 194.4614 - val_accuracy: 0.8217\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 1s 427ms/step - loss: 128.0191 - accuracy: 0.8477 - val_loss: 194.4747 - val_accuracy: 0.8217\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 1s 443ms/step - loss: 128.3213 - accuracy: 0.8469 - val_loss: 194.4700 - val_accuracy: 0.8217\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 129.1037 - accuracy: 0.8451 - val_loss: 194.4661 - val_accuracy: 0.8217\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 129.4294 - accuracy: 0.8461 - val_loss: 194.4594 - val_accuracy: 0.8217\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 128.8571 - accuracy: 0.8469 - val_loss: 194.4928 - val_accuracy: 0.8217\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 128.9357 - accuracy: 0.8465 - val_loss: 194.5115 - val_accuracy: 0.8217\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 129.3895 - accuracy: 0.8455 - val_loss: 194.5007 - val_accuracy: 0.8217\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 129.0240 - accuracy: 0.8460 - val_loss: 194.5098 - val_accuracy: 0.8217\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 1s 441ms/step - loss: 128.8308 - accuracy: 0.8465 - val_loss: 194.4924 - val_accuracy: 0.8217\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 129.0799 - accuracy: 0.8462 - val_loss: 194.4939 - val_accuracy: 0.8217\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 1s 424ms/step - loss: 129.2651 - accuracy: 0.8454 - val_loss: 194.5144 - val_accuracy: 0.8217\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 1s 442ms/step - loss: 129.0198 - accuracy: 0.8459 - val_loss: 194.5208 - val_accuracy: 0.8217\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 129.3405 - accuracy: 0.8448 - val_loss: 194.5121 - val_accuracy: 0.8217\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 128.4406 - accuracy: 0.8466 - val_loss: 194.5074 - val_accuracy: 0.8217\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 1s 440ms/step - loss: 129.0854 - accuracy: 0.8456 - val_loss: 194.5094 - val_accuracy: 0.8217\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 128.8493 - accuracy: 0.8454 - val_loss: 194.5101 - val_accuracy: 0.8217\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 130.3079 - accuracy: 0.8450 - val_loss: 194.4984 - val_accuracy: 0.8217\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 129.3760 - accuracy: 0.8446 - val_loss: 194.4888 - val_accuracy: 0.8217\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 1s 447ms/step - loss: 129.0135 - accuracy: 0.8461 - val_loss: 194.4767 - val_accuracy: 0.8217\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 129.6721 - accuracy: 0.8459 - val_loss: 194.4794 - val_accuracy: 0.8217\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 1s 431ms/step - loss: 128.9617 - accuracy: 0.8463 - val_loss: 194.4744 - val_accuracy: 0.8217\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 1s 440ms/step - loss: 130.3052 - accuracy: 0.8442 - val_loss: 194.4903 - val_accuracy: 0.8186\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 1s 428ms/step - loss: 130.2488 - accuracy: 0.8441 - val_loss: 194.5087 - val_accuracy: 0.8202\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 129.4897 - accuracy: 0.8447 - val_loss: 194.5099 - val_accuracy: 0.8202\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 1s 427ms/step - loss: 127.6454 - accuracy: 0.8465 - val_loss: 194.4881 - val_accuracy: 0.8202\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 1s 447ms/step - loss: 129.4691 - accuracy: 0.8455 - val_loss: 194.4843 - val_accuracy: 0.8202\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 1s 449ms/step - loss: 128.5120 - accuracy: 0.8466 - val_loss: 194.4819 - val_accuracy: 0.8217\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 128.3960 - accuracy: 0.8466 - val_loss: 194.4655 - val_accuracy: 0.8217\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 1s 442ms/step - loss: 128.2975 - accuracy: 0.8469 - val_loss: 194.4791 - val_accuracy: 0.8217\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 1s 428ms/step - loss: 130.0697 - accuracy: 0.8455 - val_loss: 194.4993 - val_accuracy: 0.8202\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 1s 441ms/step - loss: 127.5929 - accuracy: 0.8471 - val_loss: 194.4802 - val_accuracy: 0.8217\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 129.1765 - accuracy: 0.8463 - val_loss: 194.4466 - val_accuracy: 0.8217\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 128.8844 - accuracy: 0.8466 - val_loss: 194.4272 - val_accuracy: 0.8217\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 128.5402 - accuracy: 0.8468 - val_loss: 194.4523 - val_accuracy: 0.8217\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 1s 442ms/step - loss: 129.3740 - accuracy: 0.8462 - val_loss: 194.4951 - val_accuracy: 0.8217\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 1s 443ms/step - loss: 129.6619 - accuracy: 0.8457 - val_loss: 194.5171 - val_accuracy: 0.8202\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 128.1454 - accuracy: 0.8464 - val_loss: 194.4980 - val_accuracy: 0.8202\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 1s 444ms/step - loss: 129.9370 - accuracy: 0.8461 - val_loss: 194.4514 - val_accuracy: 0.8202\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 128.8736 - accuracy: 0.8461 - val_loss: 194.4399 - val_accuracy: 0.8202\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 129.5949 - accuracy: 0.8463 - val_loss: 194.4326 - val_accuracy: 0.8202\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 1s 445ms/step - loss: 127.6702 - accuracy: 0.8470 - val_loss: 194.4734 - val_accuracy: 0.8202\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 129.7096 - accuracy: 0.8449 - val_loss: 194.4990 - val_accuracy: 0.8202\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 130.1695 - accuracy: 0.8452 - val_loss: 194.4966 - val_accuracy: 0.8202\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 1s 444ms/step - loss: 128.3340 - accuracy: 0.8467 - val_loss: 194.4830 - val_accuracy: 0.8202\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 128.6479 - accuracy: 0.8466 - val_loss: 194.4577 - val_accuracy: 0.8217\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 128.5686 - accuracy: 0.8468 - val_loss: 194.4724 - val_accuracy: 0.8217\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 128.1252 - accuracy: 0.8465 - val_loss: 194.4815 - val_accuracy: 0.8202\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 1s 441ms/step - loss: 128.8989 - accuracy: 0.8455 - val_loss: 194.4798 - val_accuracy: 0.8202\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 128.6595 - accuracy: 0.8464 - val_loss: 194.4906 - val_accuracy: 0.8202\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 130.1539 - accuracy: 0.8451 - val_loss: 194.4542 - val_accuracy: 0.8217\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 1s 441ms/step - loss: 129.0783 - accuracy: 0.8469 - val_loss: 194.4556 - val_accuracy: 0.8217\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 1s 444ms/step - loss: 129.3333 - accuracy: 0.8458 - val_loss: 194.4763 - val_accuracy: 0.8202\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 129.1484 - accuracy: 0.8454 - val_loss: 194.4564 - val_accuracy: 0.8202\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 128.9533 - accuracy: 0.8459 - val_loss: 194.4435 - val_accuracy: 0.8202\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 1s 443ms/step - loss: 130.4006 - accuracy: 0.8448 - val_loss: 194.4467 - val_accuracy: 0.8202\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 127.9748 - accuracy: 0.8472 - val_loss: 194.4583 - val_accuracy: 0.8202\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 1s 443ms/step - loss: 128.7228 - accuracy: 0.8459 - val_loss: 194.4799 - val_accuracy: 0.8202\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 1s 439ms/step - loss: 130.3257 - accuracy: 0.8451 - val_loss: 194.4692 - val_accuracy: 0.8202\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 1s 446ms/step - loss: 129.3820 - accuracy: 0.8458 - val_loss: 194.4650 - val_accuracy: 0.8202\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 1s 442ms/step - loss: 128.4916 - accuracy: 0.8459 - val_loss: 194.4742 - val_accuracy: 0.8202\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 1s 444ms/step - loss: 129.6903 - accuracy: 0.8462 - val_loss: 194.4720 - val_accuracy: 0.8202\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 1s 448ms/step - loss: 128.7110 - accuracy: 0.8464 - val_loss: 194.4590 - val_accuracy: 0.8202\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 1s 423ms/step - loss: 128.8395 - accuracy: 0.8461 - val_loss: 194.4416 - val_accuracy: 0.8202\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 129.3621 - accuracy: 0.8454 - val_loss: 194.4385 - val_accuracy: 0.8202\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 1s 431ms/step - loss: 128.9295 - accuracy: 0.8466 - val_loss: 194.4351 - val_accuracy: 0.8202\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 1s 449ms/step - loss: 128.0759 - accuracy: 0.8468 - val_loss: 194.4214 - val_accuracy: 0.8202\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 128.4211 - accuracy: 0.8465 - val_loss: 194.4215 - val_accuracy: 0.8202\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 130.5349 - accuracy: 0.8453 - val_loss: 194.4357 - val_accuracy: 0.8202\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 1s 446ms/step - loss: 128.8757 - accuracy: 0.8450 - val_loss: 194.4650 - val_accuracy: 0.8202\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 128.6381 - accuracy: 0.8459 - val_loss: 194.4780 - val_accuracy: 0.8202\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 128.6915 - accuracy: 0.8460 - val_loss: 194.4582 - val_accuracy: 0.8202\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 1s 427ms/step - loss: 127.6636 - accuracy: 0.8471 - val_loss: 194.4349 - val_accuracy: 0.8202\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 1s 444ms/step - loss: 129.6871 - accuracy: 0.8451 - val_loss: 194.4265 - val_accuracy: 0.8202\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 129.7368 - accuracy: 0.8455 - val_loss: 194.4422 - val_accuracy: 0.8202\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 128.8948 - accuracy: 0.8461 - val_loss: 194.4650 - val_accuracy: 0.8202\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 1s 443ms/step - loss: 128.7433 - accuracy: 0.8456 - val_loss: 194.4619 - val_accuracy: 0.8202\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 129.5295 - accuracy: 0.8465 - val_loss: 194.4582 - val_accuracy: 0.8202\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 128.3685 - accuracy: 0.8461 - val_loss: 194.4439 - val_accuracy: 0.8202\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 1s 428ms/step - loss: 128.9501 - accuracy: 0.8458 - val_loss: 194.4353 - val_accuracy: 0.8202\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 1s 448ms/step - loss: 128.1322 - accuracy: 0.8467 - val_loss: 194.4502 - val_accuracy: 0.8202\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 128.3063 - accuracy: 0.8463 - val_loss: 194.4701 - val_accuracy: 0.8176\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 1s 442ms/step - loss: 128.7194 - accuracy: 0.8454 - val_loss: 194.4869 - val_accuracy: 0.8151\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 1s 444ms/step - loss: 128.1881 - accuracy: 0.8448 - val_loss: 194.4910 - val_accuracy: 0.8177\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 130.1071 - accuracy: 0.8454 - val_loss: 194.4694 - val_accuracy: 0.8177\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 1s 447ms/step - loss: 128.6588 - accuracy: 0.8453 - val_loss: 194.4349 - val_accuracy: 0.8183\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 128.6742 - accuracy: 0.8464 - val_loss: 194.4174 - val_accuracy: 0.8183\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 1s 445ms/step - loss: 129.5400 - accuracy: 0.8456 - val_loss: 194.4147 - val_accuracy: 0.8183\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 129.2865 - accuracy: 0.8456 - val_loss: 194.4381 - val_accuracy: 0.8167\n",
            "Accuracy: 84.01%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "_lrSLLX83I20",
        "outputId": "1d6bf447-ba88-47f4-e596-67e075996d78"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3wU5d338c+PIGAAUU6CBAhUFLHKKULFqviUPoJVKBZbIrcF7V0EtVZb64PF+lAtrVZbra+qbXrjoUBvwNqbYgvVYj20HokIKiAaNUgQkEYFFJEErvuPazbZ7Owmm9NuJn7fr9e+Mjtz7cxvZ2e/O3PNZNecc4iISPS1yXYBIiLSNBToIiKthAJdRKSVUKCLiLQSCnQRkVZCgS4i0koo0FsxM1tlZtObum02mVmpmY1rhvk6Mzs2GP6Nmf0onbYNWM40M3u0oXWK1MZ0HXrLYmYfxd3NBT4FDgb3L3XOLc58VS2HmZUC/+mcW93E83XAIOdcSVO1NbN84G3gMOdcZVPUKVKbttkuQGpyznWKDdcWXmbWViEhLYW2x5ZBXS4RYWZjzazMzP6fme0A7jOzo8zsL2a2y8w+CIbz4h7zhJn9ZzA8w8z+ZWa3BW3fNrMJDWw7wMyeMrO9ZrbazO4ys0Up6k6nxpvM7Olgfo+aWfe46ReZ2RYzKzezubWsn9FmtsPMcuLGTTazl4PhUWb2rJl9aGbbzezXZtYuxbzuN7OfxN3/QfCYd83skoS2XzGzl8xsj5ltNbN5cZOfCv5+aGYfmdmpsXUb9/gxZrbGzHYHf8eku27quZ67mtl9wXP4wMyWx02bZGbrgufwppmND8bX6N4ys3mx19nM8oOup2+Z2TvAP4LxDwavw+5gGzkx7vGHm9kvgtdzd7CNHW5mfzWz7yQ8n5fNbHKy5yqpKdCjpRfQFegPzMS/fvcF9/sBnwC/ruXxo4HNQHfg58ACM7MGtP0D8ALQDZgHXFTLMtOp8ULgYqAn0A64BsDMhgD3BPM/JlheHkk4554HPgb+T8J8/xAMHwSuDp7PqcCXgMtqqZughvFBPV8GBgGJ/fcfA98EjgS+Asw2s68G084I/h7pnOvknHs2Yd5dgb8CdwbP7ZfAX82sW8JzCK2bJOpazwvxXXgnBvO6PahhFPB74AfBczgDKE21PpI4EzgBODu4vwq/nnoCa4H4LsLbgJHAGPx2fC1wCHgA+I9YIzMbCvTBrxupD+ecbi30hn9jjQuGxwIHgA61tB8GfBB3/wl8lw3ADKAkblou4IBe9WmLD4tKIDdu+iJgUZrPKVmN18fdvwz4WzB8A7AkblrHYB2MSzHvnwD3BsOd8WHbP0Xbq4D/ibvvgGOD4fuBnwTD9wI3x7U7Lr5tkvneAdweDOcHbdvGTZ8B/CsYvgh4IeHxzwIz6lo39VnPQG98cB6VpN1vY/XWtv0F9+fFXue45zawlhqODNp0wX/gfAIMTdKuA/AB/rwE+OC/O9Pvt9Zw0x56tOxyzu2P3TGzXDP7bXAIuwd/iH9kfLdDgh2xAefcvmCwUz3bHgO8HzcOYGuqgtOscUfc8L64mo6Jn7dz7mOgPNWy8Hvj55tZe+B8YK1zbktQx3FBN8SOoI6f4vfW61KjBmBLwvMbbWaPB10du4FZac43Nu8tCeO24PdOY1KtmxrqWM998a/ZB0ke2hd4M816k6laN2aWY2Y3B902e6je0+8e3DokW1awTS8F/sPM2gCF+CMKqScFerQkXpL0feB4YLRz7giqD/FTdaM0he1AVzPLjRvXt5b2jalxe/y8g2V2S9XYObcRH4gTqNndAr7r5jX8XuARwA8bUgP+CCXeH4AVQF/nXBfgN3HzresSsnfxXSTx+gHb0qgrUW3reSv+NTsyyeO2Ap9LMc+P8UdnMb2StIl/jhcCk/DdUl3we/GxGv4N7K9lWQ8A0/BdYftcQveUpEeBHm2d8YexHwb9sf+/uRcY7PEWA/PMrJ2ZnQqc10w1/hE418y+GJzAvJG6t9k/AN/FB9qDCXXsAT4ys8HA7DRrWAbMMLMhwQdKYv2d8Xu/+4P+6Avjpu3Cd3UMTDHvlcBxZnahmbU1s28AQ4C/pFlbYh1J17Nzbju+b/vu4OTpYWYWC/wFwMVm9iUza2NmfYL1A7AOmBq0LwCmpFHDp/ijqFz8UVCshkP47qtfmtkxwd78qcHRFEGAHwJ+gfbOG0yBHm13AIfj936eA/6WoeVOw59YLMf3Wy/Fv5GTaXCNzrkNwOX4kN6O72ctq+Nh/40/UfcP59y/48Zfgw/bvcDvgprTqWFV8Bz+AZQEf+NdBtxoZnvxff7L4h67D5gPPG3+6povJMy7HDgXv3ddjj9JeG5C3emqaz1fBFTgj1Lew59DwDn3Av6k6+3AbuBJqo8afoTfo/4A+DE1j3iS+T3+CGkbsDGoI941wCvAGuB94BZqZtDvgZPw52SkAfSPRdJoZrYUeM051+xHCNJ6mdk3gZnOuS9mu5ao0h661JuZnWJmnwsO0cfj+02X1/U4kVSC7qzLgKJs1xJlCnRpiF74S+o+wl9DPds591JWK5LIMrOz8ecbdlJ3t47UQl0uIiKthPbQRURaiax9OVf37t1dfn5+thYvIhJJL7744r+dcz2STctaoOfn51NcXJytxYuIRJKZJf53cRV1uYiItBIKdBGRVkKBLiLSSijQRURaCQW6iEgrEdlAX7wY8vOhTRv/d/HimuPNoG3bmn/j24mItDrZ+mWNkSNHuoZatMi53FznoPqWm+vcl77knFnN8alu/fv7+TSXRYv8MiD9muq6tWnTtPPTcrQcLSfzy4lNb2gGAcXOpfiVqFQTmvvWmECPBWVT3eq7Yhctcq5bt8xvKLrpplvruuXm1j/Uawv0rH2XS0FBgWvoPxa1aeNXh4hI1PXvD6Wl6bc3sxedcwXJpkWyD71f4o+AiYhE1DvvNN28Ihno8+dDbm7d7UREWrqm3EGNZKBPmwaDBmW7ChGRxsnN9TuoTSWSgQ6wZYvvS6+NGRx2GHzxi9Ar2e+VR0zs+Vo6v1Wv5Wg5Wk6LXE5sev/+UFTkd1CbSta+bbGxDh3yt1T694fVq+HOO+Hee+Hjj6GwEM45B66/3n8gNJU2bXwtZtUna2Pj+vf3n8BN+aLFOAcbNsC+fU0/bwk74QTo3LnmuO3bYevW7NRTl169aj+c37sXNm+ufh+1b++f48GDsGkTVFaGH5OTA4MHQ8eO9avFOXjrLSgvr15Ou3bV0z/91C/zwAHo0aP6f0mkfiId6OA3rI8/rjktdhhz7LE+0G+6CX76U/j5z2HMmOozyosXw9y5Ptzjwzgd3brBr36VPKidgxdfhA8+gLw8v/E21sGD1fWVl8PChfBf/+XfkJIZHTvCeedVh3pZGTzySO07FtlkBpMmQUWF307it+9Dh/x2n1h7hw5+W6uoSD3fnBz/QVHXEXK8Dz/02238co45pvp9t22bD/WYHj3giCPSn3825ebC174GF1/s10tpKbzxhv9A3LnTf3DGdO0Kp5/efBd2RPKyRfBvrhEjoKQEduzwn/YHDqTeI3YOzj0XHnvM3047rZFPILB/v9/jf/ppv+dxySXw/PNw993VbUaPhrFj/YvYti2ceaYf/utf4d//hqOO8rUl7vUcPOgDo6gI/vIXfz/emDEwYwb06dM0z0VSO3AAHn4YHn20+nXo2BEuuMBvSy1xb/Lpp+G3v4Xu3WHYML/txRs40L+HYnvKe/bACy/4bspTTkl+4cGnn/qdlfpcZgdw+OF+nnl51ct5773q6b16wahRfp1u3QrFxf69FQXvvguPP+63gc9/Hl55pe7H3HEHfPe7DVtebZctphXowS+7/wrIAf7LOXdzwvR+wAPAkUGbOc65lbXNs7GBfvjhcOWVcPPN8PbbcPTRdR8G7tzp+9O3bIGf/cyH71FHVU/fvx92766+36lT7fN89lm4/HJ46SUf0rt2wcaNftr3vw9f/arfMBcu9C9y/F5Pp07w0UfV9484AiZPhnHj/Bvsuefgj3/0G3fPnr67qHt337ZdO/8BMGRIeutKRJrX22/7rt1//APGj/c7cG3b+lw64ojqD/ytW+Gpp/z7vKHv30YFupnlAK8DXwbKgDVAoXNuY1ybIuAl59w9ZjYEWOmcy69tvo0N9Pbt4Xvf88FcH++/Dxde6Pd827WDU0/13Sf/+lfNPYbYMr7zHbjuOn+oVFzsg7msDFat8oHesycsWOAD9tAh+POf/dHA+efXnFdFhT/k3LcPli2DN9+EqVPhxBP94dmCBbBihe+mAV/buHH+Q+e882r2N4rIZ1dtgZ5OH/oooMQ591YwsyXAJGBjXBsHxHq8ugDvNrzc9Bw6VL8+vJiuXX0Yr10LS5b4T9R33vGfqscd5/fYY5+mzz0Hv/iF76v+whfgb3+rns/IkXDbbTBrVvVefJs2fi87mcMOq77SZs6cmtN69fL9ahUV8Prr/rnl54dPwImI1CadQO8DxJ/HLwNGJ7SZBzxqZt8BOgLjmqS6WjQ00MEH9siR/lab2bPhmmt8AP/rX3DDDTB9ut+j79KlYcuuzWGH+T12EZGGaKqrXAqB+51zvzCzU4GFZvZ551yNc+hmNhOYCdCvkad5GxPo9XHSSf7kpYhIS5dOJG4D+sbdzwvGxfsWsAzAOfcs0AHonjgj51yRc67AOVfQo0ePhlVMzWu9RUTESycS1wCDzGyAmbUDpgIrEtq8A3wJwMxOwAf6rqYsNF7s2lkFuohItToj0TlXCVwBPAJsApY55zaY2Y1mNjFo9n3g22a2HvhvYIZrxgvcFegiImFp9aEH15SvTBh3Q9zwRqCJ/lWnbgp0EZGwSEaiAl1EJCySkahAFxEJi2QkKtBFRMIiGYkKdBGRsEhGogJdRCQskpGoQBcRCYtkJCrQRUTCIhmJCnQRkbBIRqICXUQkLJKRqEAXEQmLZCTGAj0nJ7t1iIi0JJEOdO2hi4hUi2QkKtBFRMIiGYkKdBGRsEhGogJdRCQskpGoQBcRCYtkJCrQRUTCIhmJCnQRkbBIRqICXUQkLJKRqEAXEQmLZCQePOj/KtBFRKpFMhK1hy4iEhbJSFSgi4iERTISFegiImGRjEQFuohIWCQjUYEuIhIWyUhUoIuIhEUyEhXoIiJhkYxEBbqISFgkI1GBLiISFslIVKCLiIRFMhIV6CIiYZGMRAW6iEhYJCNRgS4iEhapSFy8GPLz4bzz/P2//S2r5YiItCiRCfTFi2HmTNiypXrc/Pl+vIiIRCjQ586Ffftqjtu/348XEZEIBfo779RvvIjIZ01agW5m481ss5mVmNmcFG2+bmYbzWyDmf2hacuEfv3qN15E5LOmzkA3sxzgLmACMAQoNLMhCW0GAdcBpznnTgSuaupC58+H3Nya4zp08ONFRCS9PfRRQIlz7i3n3AFgCTApoc23gbuccx8AOOfea9oyYdo0KCqC/v2rx910kx8vIiLpBXofYGvc/bJgXLzjgOPM7Gkze87MxjdVgfGmTYPSUli40N//6lebYykiItHUtgnnMwgYC+QBT5nZSc65D+MbmdlMYCZAv0Z0fusfi0REwtKJxG1A37j7ecG4eGXACudchXPubeB1fMDX4Jwrcs4VOOcKevTo0dCaFegiIkmkE4lrgEFmNsDM2gFTgRUJbZbj984xs+74Lpi3mrDOGhToIiJhdUaic64SuAJ4BNgELHPObTCzG81sYtDsEaDczDYCjwM/cM6VN1fRCnQRkbC0+tCdcyuBlQnjbogbdsD3gluzU6CLiIRFMhIV6CIiYZGMRAW6iEhYJCNRgS4iEhbJSFSgi4iERTISFegiImGRjEQFuohIWCQjUYEuIhIWyUhUoIuIhEUyEhXoIiJhkYxEBbqISFgkI1GBLiISFslIVKCLiIRFMhJjgW6W3TpERFqSyAa6mQJdRCReZANd3S0iIjVFMhYV6CIiYZGMRQW6iEhYJGNRgS4iEhbJWFSgi4iERTIWFegiImGRjEUFuohIWCRjUYEuIhIWyVhUoIuIhEUyFhXoIiJhkYxFBbqISFgkY1GBLiISFslYVKCLiIRFMhYV6CIiYZGMxUOHICcn21WIiLQskQ107aGLiNQUyVhUoIuIhEUyFhXoIiJhkYxFBbqISFgkY/HgQQW6iEiiSMai9tBFRMIiGYsKdBGRsEjGogJdRCQskrGoQBcRCUsrFs1svJltNrMSM5tTS7uvmZkzs4KmKzFMgS4iElZnLJpZDnAXMAEYAhSa2ZAk7ToD3wWeb+oiEynQRUTC0onFUUCJc+4t59wBYAkwKUm7m4BbgP1NWF9SCnQRkbB0YrEPsDXuflkwroqZjQD6Ouf+WtuMzGymmRWbWfGuXbvqXWyMAl1EJKzRsWhmbYBfAt+vq61zrsg5V+CcK+jRo0eDl6lAFxEJSycWtwF94+7nBeNiOgOfB54ws1LgC8CK5jwxqkAXEQlLJxbXAIPMbICZtQOmAitiE51zu51z3Z1z+c65fOA5YKJzrrhZKkaBLiKSTJ2x6JyrBK4AHgE2AcuccxvM7EYzm9jcBSajQBcRCWubTiPn3EpgZcK4G1K0Hdv4smqnQBcRCYtkLCrQRUTCIhmLCnQRkbBIxqICXUQkLJKxqEAXEQmLZCwq0EVEwiIZiwp0EZGwSMaiAl1EJCySsahAFxEJi2QsKtBFRMIiGYsKdBGRsEjGogJdRCQskrGoQBcRCYtkLCrQRUTCIhmLCnQRkbBIxqICXUQkLJKxqEAXEQmLZCwq0EVEwiIZiwp0EZGwSMaiAl1EJCySsahAFxEJi2QsKtBFRMIiGYsKdBGRsEjGogJdRCQskrGoQBcRCYtkLCrQRUTCIhmLCnQRkbBIxqICXUQkLJKxqEAXEQmLZCwq0EVEwiIXi875vwp0EZGaIheLhw75vwp0EZGaIheLCnQRkeQiF4sKdBGR5CIXiwp0EZHkIheLCnQRkeQiF4uxQP/Zz3yo5+fD4sVZLUlEpEVom+0C6mvJEv/3gw/83y1bYOZMPzxtWnZqEhFpCSK3h37jjeFx+/bB3LmZr0VEpCVJK9DNbLyZbTazEjObk2T698xso5m9bGaPmVn/pi/VKytLPv6dd5priSIi0VBnoJtZDnAXMAEYAhSa2ZCEZi8BBc65k4E/Aj9v6kJj8vKSj+/Xr7mWKCISDensoY8CSpxzbznnDgBLgEnxDZxzjzvn9gV3nwNSxG7jXXddeFxuLsyf31xLFBGJhnQCvQ+wNe5+WTAulW8Bq5JNMLOZZlZsZsW7du1Kv8o4kyf7v127ghn07w9FRTohKiLSpFe5mNl/AAXAmcmmO+eKgCKAgoIC15BlxC5bvPlm+Pa3G1SmiEirlE6gbwP6xt3PC8bVYGbjgLnAmc65T5umvDD9Y5GISHLpxOIaYJCZDTCzdsBUYEV8AzMbDvwWmOice6/py6ymQBcRSa7OWHTOVQJXAI8Am4BlzrkNZnajmU0Mmt0KdAIeNLN1ZrYixewaTYEuIpJcWn3ozrmVwMqEcTfEDY9r4rpSOnjQ/83JydQSRUSiIXL7udpDFxFJLnKxqEAXEUkucrGoQBcRSS5ysahAFxFJLnKxqEAXEUkucrGoQBcRSS5ysahAFxFJLnKxqEAXEUkucrGoQBcRSS5ysahAFxFJLnKxqEAXEUkucrGoQBcRSa5Jf+AiExToIo1XUVFBWVkZ+/fvz3YpkkKHDh3Iy8vjsMMOS/sxCnSRz6CysjI6d+5Mfn4+ZpbtciSBc47y8nLKysoYMGBA2o+LXCwq0EUab//+/XTr1k1h3kKZGd26dav3EVTkYlGBLtI0FOYtW0Nen8jFogJdRCS5yMWiAl0k8xYvhvx8/77Lz/f3G6O8vJxhw4YxbNgwevXqRZ8+faruHzhwoNbHFhcXc+WVV9a5jDFjxjSuyAjSSVERqdXixTBzJuzb5+9v2eLvA0yb1rB5duvWjXXr1gEwb948OnXqxDXXXFM1vbKykrZtk8dTQUEBBQUFdS7jmWeeaVhxERa5WFSgi2TW3LnVYR6zb58f35RmzJjBrFmzGD16NNdeey0vvPACp556KsOHD2fMmDFs3rwZgCeeeIJzzz0X8B8Gl1xyCWPHjmXgwIHceeedVfPr1KlTVfuxY8cyZcoUBg8ezLRp03DOAbBy5UoGDx7MyJEjufLKK6vmG6+0tJTTTz+dESNGMGLEiBofFLfccgsnnXQSQ4cOZc6cOQCUlJQwbtw4hg4dyogRI3jzzTebdkXVQnvoIlKrd96p3/jGKCsr45lnniEnJ4c9e/bwz3/+k7Zt27J69Wp++MMf8tBDD4Ue89prr/H444+zd+9ejj/+eGbPnh26dvull15iw4YNHHPMMZx22mk8/fTTFBQUcOmll/LUU08xYMAACgsLk9bUs2dP/v73v9OhQwfeeOMNCgsLKS4uZtWqVfz5z3/m+eefJzc3l/fffx+AadOmMWfOHCZPnsz+/fs5FAutDFCgi0it+vXz3SzJxje1Cy64gJycHAB2797N9OnTeeONNzAzKioqkj7mK1/5Cu3bt6d9+/b07NmTnTt3kpeXV6PNqFGjqsYNGzaM0tJSOnXqxMCBA6uu8y4sLKSoqCg0/4qKCq644grWrVtHTk4Or7/+OgCrV6/m4osvJjc3F4CuXbuyd+9etm3bxuTJkwH/z0GZFLlYVKCLZNb8+RBkVpXcXD++qXXs2LFq+Ec/+hFnnXUWr776Kg8//HDKa7Lbt29fNZyTk0NlZWWD2qRy++23c/TRR7N+/XqKi4vrPGmbTZGLRQW6SGZNmwZFRdC/P5j5v0VFDT8hmq7du3fTp08fAO6///4mn//xxx/PW2+9RWlpKQBLly5NWUfv3r1p06YNCxcu5ODBgwB8+ctf5r777mNfcILh/fffp3PnzuTl5bF8+XIAPv3006rpmRC5WFSgi2TetGlQWurff6WlzR/mANdeey3XXXcdw4cPr9cedboOP/xw7r77bsaPH8/IkSPp3LkzXbp0CbW77LLLeOCBBxg6dCivvfZa1VHE+PHjmThxIgUFBQwbNozbbrsNgIULF3LnnXdy8sknM2bMGHbs2NHktadisbO9mVZQUOCKi4vr/bglS6CwEDZtgsGDm6Ewkc+ATZs2ccIJJ2S7jKz76KOP6NSpE845Lr/8cgYNGsTVV1+d7bKqJHudzOxF51zS6zYjt5+rPXQRaSq/+93vGDZsGCeeeCK7d+/m0ksvzXZJjaKrXETkM+vqq69uUXvkjRW5WFSgi4gkF7lYVKCLiCQXuVhUoIuIJBe5WFSgi4gkF7lYVKCLRN9ZZ53FI488UmPcHXfcwezZs1M+ZuzYscQudT7nnHP48MMPQ23mzZtXdT14KsuXL2fjxo1V92+44QZWr15dn/JbrMjFogJdJPoKCwtZsmRJjXFLlixJ+QVZiVauXMmRRx7ZoGUnBvqNN97IuHHjGjSvlkaXLYp8xl11FQRfTd5khg2DO+5IPX3KlClcf/31HDhwgHbt2lFaWsq7777L6aefzuzZs1mzZg2ffPIJU6ZM4cc//nHo8fn5+RQXF9O9e3fmz5/PAw88QM+ePenbty8jR44E/DXmRUVFHDhwgGOPPZaFCxeybt06VqxYwZNPPslPfvITHnroIW666SbOPfdcpkyZwmOPPcY111xDZWUlp5xyCvfccw/t27cnPz+f6dOn8/DDD1NRUcGDDz7I4IT/bCwtLeWiiy7i448/BuDXv/511Y9s3HLLLSxatIg2bdowYcIEbr75ZkpKSpg1axa7du0iJyeHBx98kM997nONWu+Ri0UFukj0de3alVGjRrFq1SrA751//etfx8yYP38+xcXFvPzyyzz55JO8/PLLKefz4osvsmTJEtatW8fKlStZs2ZN1bTzzz+fNWvWsH79ek444QQWLFjAmDFjmDhxIrfeeivr1q2rEaD79+9nxowZLF26lFdeeYXKykruueeequndu3dn7dq1zJ49O2m3TuxrdteuXcvSpUurflUp/mt2169fz7XXXgv4r9m9/PLLWb9+Pc888wy9e/du3EpFe+gin3m17Uk3p1i3y6RJk1iyZAkLFiwAYNmyZRQVFVFZWcn27dvZuHEjJ598ctJ5/POf/2Ty5MlVX2E7ceLEqmmvvvoq119/PR9++CEfffQRZ599dq31bN68mQEDBnDccccBMH36dO666y6uuuoqwH9AAIwcOZI//elPoce3hK/ZVaCLSFZMmjSJq6++mrVr17Jv3z5GjhzJ22+/zW233caaNWs46qijmDFjRsqvza3LjBkzWL58OUOHDuX+++/niSeeaFS9sa/gTfX1u/Ffs3vo0KGMfxc6pNnlYmbjzWyzmZWY2Zwk09ub2dJg+vNmlt/UhcYo0EVah06dOnHWWWdxySWXVJ0M3bNnDx07dqRLly7s3LmzqksmlTPOOIPly5fzySefsHfvXh5++OGqaXv37qV3795UVFSwOO5XrTt37szevXtD8zr++OMpLS2lpKQE8N+aeOaZZ6b9fFrC1+zWGYtmlgPcBUwAhgCFZjYkodm3gA+cc8cCtwO3NLqyFBToIq1HYWEh69evrwr0oUOHMnz4cAYPHsyFF17IaaedVuvjR4wYwTe+8Q2GDh3KhAkTOOWUU6qm3XTTTYwePZrTTjutxgnMqVOncuuttzJ8+PAav/fZoUMH7rvvPi644AJOOukk2rRpw6xZs9J+Li3ha3br/PpcMzsVmOecOzu4fx2Ac+5ncW0eCdo8a2ZtgR1AD1fLzBv69bkrVsCiRfD730MWjmhEWgV9fW40NMfX5/YBtsbdLwvGJW3jnKsEdgPdEmdkZjPNrNjMinft2pXGosMmToRlyxTmIiKJMtpx4Zwrcs4VOOcKevTokclFi4i0eukE+jagb9z9vGBc0jZBl0sXoLwpChSR5pGtXyuT9DTk9Ukn0NcAg8xsgJm1A6YCKxLarACmB8NTgH/U1n8uItnVoUMHysvLFeotlHOO8vLyel/6WOd16M65SjO7AngEyAHudc5tMLMbgWLn3ApgAbDQzEqA9/GhLyItVF5eHmVlZTT0XJY0vw4dOoRwnpcAAATMSURBVJCXl1evx0TuR6JFRD7LWtWPRIuISHIKdBGRVkKBLiLSSmStD93MdgFbGvjw7sC/m7CcptRSa1Nd9aO66q+l1tba6urvnEv6jzxZC/TGMLPiVCcFsq2l1qa66kd11V9Lre2zVJe6XEREWgkFuohIKxHVQC/KdgG1aKm1qa76UV3111Jr+8zUFck+dBERCYvqHrqIiCRQoIuItBKRC/S6ft80g3X0NbPHzWyjmW0ws+8G4+eZ2TYzWxfczslCbaVm9kqw/OJgXFcz+7uZvRH8PSrDNR0ft07WmdkeM7sqW+vLzO41s/fM7NW4cUnXkXl3Btvcy2Y2IsN13WpmrwXL/h8zOzIYn29mn8Stu99kuK6Ur52ZXResr81mdnZz1VVLbUvj6io1s3XB+Iyss1ryoXm3MedcZG74b3t8ExgItAPWA0OyVEtvYEQw3Bl4Hf+bq/OAa7K8nkqB7gnjfg7MCYbnALdk+XXcAfTP1voCzgBGAK/WtY6Ac4BVgAFfAJ7PcF3/F2gbDN8SV1d+fLssrK+kr13wPlgPtAcGBO/ZnEzWljD9F8ANmVxnteRDs25jUdtDHwWUOOfecs4dAJYAk7JRiHNuu3NubTC8F9hE+Kf5WpJJwAPB8APAV7NYy5eAN51zDf1P4UZzzj2F/6rneKnW0STg9857DjjSzHpnqi7n3KPO/7QjwHP4H5nJqBTrK5VJwBLn3KfOubeBEvx7N+O1mZkBXwf+u7mWn6KmVPnQrNtY1AI9nd83zTgzyweGA88Ho64IDpvuzXTXRsABj5rZi2Y2Mxh3tHNuezC8Azg6C3XFTKXmGyzb6ysm1TpqSdvdJfg9uZgBZvaSmT1pZqdnoZ5kr11LWl+nAzudc2/EjcvoOkvIh2bdxqIW6C2OmXUCHgKucs7tAe4BPgcMA7bjD/cy7YvOuRHABOByMzsjfqLzx3hZuV7V/K9eTQQeDEa1hPUVks11lIqZzQUqgcXBqO1AP+fccOB7wB/M7IgMltQiX7sEhdTcecjoOkuSD1WaYxuLWqCn8/umGWNmh+FfrMXOuT8BOOd2OucOOucOAb+jGQ81U3HObQv+vgf8T1DDztghXPD3vUzXFZgArHXO7QxqzPr6ipNqHWV9uzOzGcC5wLQgCAi6NMqD4RfxfdXHZaqmWl67rK8vqPp94/OBpbFxmVxnyfKBZt7Gohbo6fy+aUYEfXMLgE3OuV/GjY/v95oMvJr42Gauq6OZdY4N40+ovUrN332dDvw5k3XFqbHHlO31lSDVOloBfDO4EuELwO64w+ZmZ2bjgWuBic65fXHje5hZTjA8EBgEvJXBulK9diuAqWbW3swGBHW9kKm64owDXnPOlcVGZGqdpcoHmnsba+6zvU19w58Nfh3/yTo3i3V8EX+49DKwLridAywEXgnGrwB6Z7iugfgrDNYDG2LrCOgGPAa8AawGumZhnXUEyoEuceOysr7wHyrbgQp8f+W3Uq0j/JUHdwXb3CtAQYbrKsH3r8a2s98Ebb8WvMbrgLXAeRmuK+VrB8wN1tdmYEKmX8tg/P3ArIS2GVlnteRDs25j+td/EZFWImpdLiIikoICXUSklVCgi4i0Egp0EZFWQoEuItJKKNBFRFoJBbqISCvxv2qDSTzTmE/1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_44LGpK-xx4"
      },
      "source": [
        "def final_predictions(text):\n",
        "  y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
        "  y_id_to_word[0] = '<PAD>'\n",
        "\n",
        "  sentence = [english_tokenizer.word_index[word] for word in text.split()]\n",
        "  sentence = pad_sequences([sentence], maxlen=preproc_french_sentences.shape[-2], padding='post')\n",
        "  \n",
        "  print(sentence.shape)\n",
        "  print(logits_to_text(simple_rnn_model.predict(sentence[:1])[0], french_tokenizer))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm-DAR9H-4xD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43179696-d46a-42a9-b27f-cc6bc638345b"
      },
      "source": [
        "cnn_model.save('model')"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onnTqdkf-_ry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a45f0a0-112a-436c-e4a0-956fb9f7be0c"
      },
      "source": [
        "! mkdir \"pickles\""
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘pickles’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhDpg-NB_AXV"
      },
      "source": [
        "with open('pickles/french_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(french_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('pickles/english_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(english_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('pickles/preproc_french_sentences.pickle', 'wb') as handle:\n",
        "    pickle.dump(preproc_french_sentences, handle, protocol=pickle.HIGHEST_PROTOCOL) "
      ],
      "execution_count": 155,
      "outputs": []
    }
  ]
}