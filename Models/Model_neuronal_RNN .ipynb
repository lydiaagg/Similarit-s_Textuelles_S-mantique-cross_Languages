{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_neuronal_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyCAqBpNCnPq"
      },
      "source": [
        "%%capture\n",
        "# Install the latest Tensorflow version.\n",
        "!pip install tensorflow_text\n",
        "!pip install bokeh\n",
        "!pip install simpleneighbors[annoy]\n",
        "!pip install tqdm\n",
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JlNt2Iu72or"
      },
      "source": [
        "import os,re,collections\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GRU,LSTM, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "import tensorflow.compat.v2 as tf\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCDCpjmaV6Iw"
      },
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "#nous allons extraire nos données qui se trouvent dans un fichier zip dans notre drive \n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/rebuilt.Cross-Language-Dataset-master.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBlohYw1Wb77"
      },
      "source": [
        "#nous allons experimenter une partie de nos donnée dans cette partie on commence par le corpus APR \n",
        "English_Dataset = tf.data.Dataset.list_files(\"/tmp/Cross-Language-Dataset-master/dataset/chunks/APR/en/*.txt\")\n",
        "Frensh_Dataset = tf.data.Dataset.list_files(\"/tmp/Cross-Language-Dataset-master/dataset/chunks/APR/fr/*.txt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH58fu-j_in7"
      },
      "source": [
        "#nous allons organiser nos fichier text de sort a ce que les fichier en francais aille leur traduction anglaise\n",
        "English_DatasetFiles=list(English_Dataset.as_numpy_iterator())\n",
        "English_DatasetFiles.sort()\n",
        "Frensh_DatasetFiles=list(Frensh_Dataset .as_numpy_iterator())\n",
        "Frensh_DatasetFiles.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtJR_FFUUQ0d"
      },
      "source": [
        "#cette fonction permet d'extraire extrait les donnees des fichier\n",
        "def load_data(Files):\n",
        "  sentences=[]\n",
        "  for file_content in Files: \n",
        "    with open(file_content, \"r\") as f:\n",
        "      data = f.read()\n",
        "      sentences.append(data)\n",
        "  return sentences\n",
        "\n",
        "english_sentences=load_data(English_DatasetFiles)\n",
        "french_sentences=load_data(Frensh_DatasetFiles)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITcf0jAdfy6u",
        "outputId": "302902d1-be3c-4a1b-e5fc-f1ef3832e254"
      },
      "source": [
        "for i in range(5):\n",
        "  print('Sample :',i)\n",
        "  print(english_sentences[i])\n",
        "  print(french_sentences[i])\n",
        "  print('-'*50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample : 0\n",
            "the association gregory lemarchal\n",
            "\n",
            "l' association gregory lemarchal\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 1\n",
            "after the dresden file\n",
            "\n",
            "après les dossier dresden\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 2\n",
            "the tradition of lanfeust troy\n",
            "invention of reverse auctions\n",
            "\n",
            "la tradition lanfeust de troy\n",
            "invention des ventes aux enchères\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 3\n",
            "the manner of dos passos\n",
            "\n",
            "la manière de dos passos\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 4\n",
            "trace of fat in this book\n",
            "\n",
            "trace de gras dans le livre\n",
            "\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gth9wlQr9j3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd95204-683e-41b0-fbe3-fc7b00ee5f83"
      },
      "source": [
        "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
        "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
        "\n",
        "print('English Vocab:',len(english_words_counter))\n",
        "print('French Vocab:',len(french_words_counter))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocab: 4036\n",
            "French Vocab: 4309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP0AOadJ9lCz"
      },
      "source": [
        "def tokenize(x):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(x)\n",
        "  return tokenizer.texts_to_sequences(x), tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL_rK09M9vJf"
      },
      "source": [
        "def pad(x, length=None):\n",
        "  return pad_sequences(x, maxlen=length, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7_d5he_9ywE"
      },
      "source": [
        "def preprocess(x, y):\n",
        "    \"\"\"\n",
        "      Prétraiter x et y\n",
        "    : param x: Feature Liste des phrases\n",
        "    : param y: Label Liste des phrases\n",
        "    : return: Tuple de (pré-traité x, pré-traité y, x tokenizer, y tokenizer)\n",
        "    \"\"\"\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # La fonction sparse_categorical_crossentropy de Keras nécessite que les étiquettes soient en 3 dimensions\n",
        "    # Extension des dimensions\n",
        "    \n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yDJo4gD96lH"
      },
      "source": [
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
        "    preprocess(english_sentences, french_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5tS4iyl-Lhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96e559d-70b3-4c9d-b167-9683355ae64b"
      },
      "source": [
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Preprocessed\n",
            "Max English sentence length: 44\n",
            "Max French sentence length: 46\n",
            "English vocabulary size: 4020\n",
            "French vocabulary size: 4294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxd8wRHV-OlA"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "  index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "  index_to_words[0] = '<PAD>'\n",
        "\n",
        "   # Nous prédisons la sortie pour un mot donné, puis sélectionnons la meilleure réponse\n",
        "   # En sélectionnant cette étiquette, nous énumérons le mot à partir de l'identifiant\n",
        "    \n",
        "  return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5qUAc6N-WKW"
      },
      "source": [
        "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "     Créer et entraîner un modèle RNN à l'aide de l'incorporation de mots sur x et y\n",
        "     \n",
        "     : param input_shape: Tuple de la forme d'entrée\n",
        "     : param output_sequence_length: Longueur de la séquence de sortie\n",
        "     : param english_vocab_size: Nombre de mots anglais uniques dans le jeu de données\n",
        "     : param french_vocab_size: Nombre de mots français uniques dans le jeu de données\n",
        "     : return: modèle Keras construit, mais pas entraîné\n",
        "     \"\"\"\n",
        "    # Hyperparamètres\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    # Couches\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
        "    model.add(LSTM(256, return_sequences=True))    \n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
        "     # Compilation du modèle\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkGroinK-g_9"
      },
      "source": [
        "# Le reshaping l'entrée pour travailler avec un RNN de base\n",
        "\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-forzESg-j94"
      },
      "source": [
        "simple_rnn_model = embed_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9KkmafL-rjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3762d5-16f9-4aa0-a33b-b6bd3a8410cd"
      },
      "source": [
        "simple_rnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 46, 256)           1029376   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 46, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 46, 1024)          263168    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 46, 1024)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 46, 4295)          4402375   \n",
            "=================================================================\n",
            "Total params: 6,220,231\n",
            "Trainable params: 6,220,231\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkmpnkOi-uI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd8b9466-1cde-45c3-e170-03efb0e63a2f"
      },
      "source": [
        "history=simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2/2 [==============================] - 52s 17s/step - loss: 8.2919 - accuracy: 0.1518 - val_loss: 2.6137 - val_accuracy: 0.8253\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 49s 16s/step - loss: 2.3835 - accuracy: 0.8388 - val_loss: 1.8763 - val_accuracy: 0.8253\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 49s 16s/step - loss: 1.7054 - accuracy: 0.8397 - val_loss: 2.0693 - val_accuracy: 0.8253\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 48s 16s/step - loss: 1.8681 - accuracy: 0.8358 - val_loss: 1.9541 - val_accuracy: 0.8242\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 48s 16s/step - loss: 2.1041 - accuracy: 0.7132 - val_loss: 1.7931 - val_accuracy: 0.8271\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 49s 16s/step - loss: 1.4936 - accuracy: 0.8417 - val_loss: 1.6038 - val_accuracy: 0.8261\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 49s 16s/step - loss: 1.3480 - accuracy: 0.8415 - val_loss: 1.4944 - val_accuracy: 0.8390\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 49s 17s/step - loss: 1.2811 - accuracy: 0.8509 - val_loss: 1.4248 - val_accuracy: 0.8417\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 49s 16s/step - loss: 1.2233 - accuracy: 0.8539 - val_loss: 1.4082 - val_accuracy: 0.8423\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 48s 16s/step - loss: 1.1934 - accuracy: 0.8545 - val_loss: 1.4225 - val_accuracy: 0.8419\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 48s 16s/step - loss: 1.1649 - accuracy: 0.8553 - val_loss: 1.3996 - val_accuracy: 0.8415\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 48s 16s/step - loss: 1.1440 - accuracy: 0.8550 - val_loss: 1.3977 - val_accuracy: 0.8419\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 49s 16s/step - loss: 1.1273 - accuracy: 0.8553 - val_loss: 1.4144 - val_accuracy: 0.8421\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 49s 16s/step - loss: 1.1199 - accuracy: 0.8543 - val_loss: 1.4107 - val_accuracy: 0.8419\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 48s 16s/step - loss: 1.1000 - accuracy: 0.8556 - val_loss: 1.4068 - val_accuracy: 0.8419\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 48s 16s/step - loss: 1.0888 - accuracy: 0.8557 - val_loss: 1.4245 - val_accuracy: 0.8420\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 48s 16s/step - loss: 1.0739 - accuracy: 0.8561 - val_loss: 1.4247 - val_accuracy: 0.8416\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 48s 16s/step - loss: 1.0599 - accuracy: 0.8568 - val_loss: 1.4180 - val_accuracy: 0.8414\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 48s 16s/step - loss: 1.0455 - accuracy: 0.8569 - val_loss: 1.4239 - val_accuracy: 0.8420\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 48s 16s/step - loss: 1.0431 - accuracy: 0.8571 - val_loss: 1.4226 - val_accuracy: 0.8418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_44LGpK-xx4"
      },
      "source": [
        "def final_predictions(text):\n",
        "  y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
        "  y_id_to_word[0] = '<PAD>'\n",
        "\n",
        "  sentence = [english_tokenizer.word_index[word] for word in text.split()]\n",
        "  sentence = pad_sequences([sentence], maxlen=preproc_french_sentences.shape[-2], padding='post')\n",
        "  \n",
        "  print(sentence.shape)\n",
        "  print(logits_to_text(simple_rnn_model.predict(sentence[:1])[0], french_tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm-DAR9H-4xD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc5c956-17ed-40b9-eefb-c9657616164a"
      },
      "source": [
        "simple_rnn_model.save('model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onnTqdkf-_ry"
      },
      "source": [
        "! mkdir \"pickles\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhDpg-NB_AXV"
      },
      "source": [
        "with open('pickles/french_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(french_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('pickles/english_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(english_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('pickles/preproc_french_sentences.pickle', 'wb') as handle:\n",
        "    pickle.dump(preproc_french_sentences, handle, protocol=pickle.HIGHEST_PROTOCOL) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}