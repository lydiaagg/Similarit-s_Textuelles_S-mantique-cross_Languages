{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recurrent_Neural_Networks_(SimpleRNN).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyCAqBpNCnPq"
      },
      "source": [
        "%%capture\n",
        "# Install the latest Tensorflow version.\n",
        "!pip install tensorflow_text\n",
        "!pip install bokeh\n",
        "!pip install simpleneighbors[annoy]\n",
        "!pip install tqdm\n",
        "!pip install unidecode"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JlNt2Iu72or"
      },
      "source": [
        "import os,re,collections\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GRU,LSTM, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "import tensorflow.compat.v2 as tf\n",
        "import pickle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vt2rQCOPz74"
      },
      "source": [
        "## ***Etape 1 : Chargement de données*** \n",
        " Nous allons extraire les données qui se trouvent dans le zip dans le drive ainsi pouvoirs les exploité facilement puis nous allons organiser les fichier de sort a ce que chaque phrase aille sa traduction dans le fichier correspondant,les phrase qui seront extraitent du fichier vont étre prétraiter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7lnrHSLa5AF"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "#nous allons extraire nos données qui se trouvent dans un fichier zip dans notre drive \n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/rebuilt.Cross-Language-Dataset-master.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBlohYw1Wb77"
      },
      "source": [
        "#nous allons experimenter une partie de nos donnée dans cette partie on commence par le corpus APR \n",
        "English_Dataset= tf.data.Dataset.list_files(\"/tmp/Cross-Language-Dataset-master/dataset/chunks/APR/en/*.txt\")\n",
        "Frensh_Dataset = tf.data.Dataset.list_files(\"/tmp/Cross-Language-Dataset-master/dataset/chunks/APR/fr/*.txt\")\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH58fu-j_in7"
      },
      "source": [
        "#nous allons organiser nos fichier text de sort a ce que les fichier en francais aille leur traduction anglaise\n",
        "English_DatasetFiles=list(English_Dataset.as_numpy_iterator())\n",
        "English_DatasetFiles.sort()\n",
        "Frensh_DatasetFiles=list(Frensh_Dataset .as_numpy_iterator())\n",
        "Frensh_DatasetFiles.sort()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtJR_FFUUQ0d"
      },
      "source": [
        "#cette fonction permet d'extraire extrait les donnees des fichier\n",
        "def load_data(Files):\n",
        "  sentences=[]\n",
        "  for file_content in Files: \n",
        "    with open(file_content, \"r\") as f:\n",
        "      data = f.read()\n",
        "      sentences.append(data)\n",
        "  return sentences\n",
        "\n",
        "english_sentences=load_data(English_DatasetFiles)\n",
        "french_sentences=load_data(Frensh_DatasetFiles)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITcf0jAdfy6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b103f48-f7f6-4822-b790-2df0a28fb0a5"
      },
      "source": [
        "for i in range(5):\n",
        "  print('Sample :',i)\n",
        "  print(english_sentences[i])\n",
        "  print(french_sentences[i])\n",
        "  print('-'*50)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample : 0\n",
            "the association gregory lemarchal\n",
            "\n",
            "l' association gregory lemarchal\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 1\n",
            "after the dresden file\n",
            "\n",
            "après les dossier dresden\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 2\n",
            "the tradition of lanfeust troy\n",
            "invention of reverse auctions\n",
            "\n",
            "la tradition lanfeust de troy\n",
            "invention des ventes aux enchères\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 3\n",
            "the manner of dos passos\n",
            "\n",
            "la manière de dos passos\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample : 4\n",
            "trace of fat in this book\n",
            "\n",
            "trace de gras dans le livre\n",
            "\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcTWq6nvOmDQ"
      },
      "source": [
        "# ***Etape 2 : Stopwords***\n",
        "première manipulation souvent effectuée dans le traitement du texte est la suppression de ce qu'on appelle en anglais les stopwords. Ce sont les mots très courants dans la langue étudiée (\"et\", \"à\", \"le\"... en français) qui n'apportent pas de valeur informative pour la compréhension du \"sens\" d'un document et corpus. Il sont très fréquents et ralentissent notre travail : nous souhaitons donc les supprimer.\n",
        "\n",
        "Il existe dans la librairie NLTK une liste par défaut des stopwords dans plusieurs Langues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0hrikyVlnAw",
        "outputId": "e7c12d48-a8ec-4aa7-ad37-6a7cb6be4fbb"
      },
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#pour supprimer les stopword de chaque langue\n",
        "def remove_stopwords(text,langue):\n",
        "    STOPWORDS = set(stopwords.words(langue))\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "#supprimer les stopword dans une liste de phrase\n",
        "def remove(sentences,langue):\n",
        "    values=[]\n",
        "    for text in sentences:\n",
        "       values.append(remove_stopwords(text,langue))\n",
        "    return values\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FFU2kwwlp3r"
      },
      "source": [
        "\n",
        "english_sentences=remove(english_sentences,'english')\n",
        "french_sentences=remove(french_sentences,'french')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gth9wlQr9j3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c0f307-e6cf-421f-de9c-34805bde3fd0"
      },
      "source": [
        "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
        "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
        "\n",
        "print('English Vocab:',len(english_words_counter))\n",
        "print('French Vocab:',len(french_words_counter))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocab: 3981\n",
            "French Vocab: 4271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGEo2THGR3hr"
      },
      "source": [
        "# ***Etape 3 :le Stemming*** \n",
        "racinisation(ou stemming en anglais). Cela consiste à ne conserver que la racine des mots étudiés. L'idée étant de supprimer les suffixes, préfixes et autres des mots afin de ne conserver que leur origine. C'est un procédé plus simple que la lemmatisation et plus rapide à effectuer puisqu'on tronque les mots essentiellement contrairement à la lemmatisation qui nécessite d'utiliser un dictionnaire.Dans notre cas, on va effectuer une racinisation parce qu'il n'existe pas de fonction de lemmatisation de corpus français dans NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3gDnA9XoD2Q"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "def stem_words_langue(text,langue):\n",
        "    stemmer2=SnowballStemmer(langue)\n",
        "    return \" \".join([stemmer2.stem(word) for word in text.split()])\n",
        "\n",
        "def stemming_langue(sentences,langue):\n",
        "  values=[]\n",
        "  for text in sentences:\n",
        "    values.append(stem_words_langue(text,langue))\n",
        "  return values\n",
        " "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zds9TdSQEnn"
      },
      "source": [
        "\n",
        "english_sentences=stemming_langue(english_sentences,'english')\n",
        "french_sentences=stemming_langue(french_sentences,'french')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfDrPIYqTiPv"
      },
      "source": [
        "## ***Etape 4 : la tokenisation***\n",
        "La tokenisation est la division du texte brut en petits morceaux de mots ou de phrases, appelés jetons.Chaque phrase prend son sens par les mots qu'elle contient. Ainsi, en analysant les mots présents dans le texte, nous pouvons facilement interpréter le sens du texte. Une fois que nous avons une liste de mots, nous pouvons également utiliser des outils et des méthodes statistiques pour mieux comprendre le texte. Par exemple, nous pouvons utiliser le nombre de mots et la fréquence des mots pour découvrir l'importance d'un mot dans cette phrase ou ce document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP0AOadJ9lCz"
      },
      "source": [
        "def tokenize(x):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(x)\n",
        "  return tokenizer.texts_to_sequences(x), tokenizer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL_rK09M9vJf"
      },
      "source": [
        "def pad(x, length=None):\n",
        "  return pad_sequences(x, maxlen=length, padding='post')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7_d5he_9ywE"
      },
      "source": [
        "def preprocess(x, y):\n",
        "    \"\"\"\n",
        "      Prétraiter x et y\n",
        "    : param x: Feature Liste des phrases\n",
        "    : param y: Label Liste des phrases\n",
        "    : return: Tuple de (pré-traité x, pré-traité y, x tokenizer, y tokenizer)\n",
        "    \"\"\"\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # La fonction sparse_categorical_crossentropy de Keras nécessite que les étiquettes soient en 3 dimensions\n",
        "    # Extension des dimensions\n",
        "    \n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yDJo4gD96lH"
      },
      "source": [
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
        "    preprocess(english_sentences, french_sentences)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5tS4iyl-Lhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1acec2e7-89b7-4611-a44e-8803c024fc95"
      },
      "source": [
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Preprocessed\n",
            "Max English sentence length: 31\n",
            "Max French sentence length: 34\n",
            "English vocabulary size: 3616\n",
            "French vocabulary size: 3766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxd8wRHV-OlA"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "  index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "  index_to_words[0] = '<PAD>'\n",
        "\n",
        "   # Nous prédisons la sortie pour un mot donné, puis sélectionnons la meilleure réponse\n",
        "   # En sélectionnant cette étiquette, nous énumérons le mot à partir de l'identifiant\n",
        "    \n",
        "  return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkGroinK-g_9"
      },
      "source": [
        "# Le reshaping l'entrée pour travailler avec un RNN de base\n",
        "\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzq41ASNU-pX"
      },
      "source": [
        "# ***Etape 5 les Word embedding*** \n",
        "Nous allons utilisée un prétrained model Glove word embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCwMiNIc56t4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f04a07d9-eb7c-42c8-fb7e-bf6cc77b66e5"
      },
      "source": [
        "glove_dir = '/content/drive/MyDrive/glove/'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByXen8xW59C4"
      },
      "source": [
        "embedding_dim = 100\n",
        "max_words = english_vocab_size+1\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in english_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EoAYcFZWPVi"
      },
      "source": [
        "# ***Etape 6 : Model***\n",
        "Nous allons maintenant procedé a la creation de notre model neuronal ,l'entrainer et l'evaluer puis afficher les resulat donné a partir d'un graphe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4AiU7VaCufE"
      },
      "source": [
        "from keras.layers import Dense\n",
        "from keras.layers import SimpleRNN\n",
        "def embed_modelSimpleRNN(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "     Créer et entraîner un modèle RNN à l'aide de l'incorporation de mots sur x et y\n",
        "     \n",
        "     : param input_shape: Tuple de la forme d'entrée\n",
        "     : param output_sequence_length: Longueur de la séquence de sortie\n",
        "     : param english_vocab_size: Nombre de mots anglais uniques dans le jeu de données\n",
        "     : param french_vocab_size: Nombre de mots français uniques dans le jeu de données\n",
        "     : return: modèle Keras construit, mais pas entraîné\n",
        "     \"\"\"\n",
        "    # Hyperparamètres\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    # Couches\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(english_vocab_size, 100, weights=[embedding_matrix],input_length=input_shape[1], input_shape=input_shape[1:]))\n",
        "    model.add(SimpleRNN(256, return_sequences=True))    \n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
        "     # Compilation du modèle\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "rnn_model = embed_modelSimpleRNN(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMIqLVr3tIXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "658a3572-c370-414f-df84-c51e166d14a3"
      },
      "source": [
        "history=rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=200, validation_split=0.2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 61s 11s/step - loss: 7.9538 - accuracy: 0.1587 - val_loss: 1.6274 - val_accuracy: 0.8287\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 30s 11s/step - loss: 1.5690 - accuracy: 0.8326 - val_loss: 1.6756 - val_accuracy: 0.8292\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 1.5073 - accuracy: 0.8455 - val_loss: 1.7509 - val_accuracy: 0.8349\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 1.4957 - accuracy: 0.8498 - val_loss: 1.7634 - val_accuracy: 0.8379\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 1.4892 - accuracy: 0.8520 - val_loss: 1.7371 - val_accuracy: 0.8367\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 32s 11s/step - loss: 1.4008 - accuracy: 0.8528 - val_loss: 1.6390 - val_accuracy: 0.8372\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 1.3140 - accuracy: 0.8519 - val_loss: 1.5564 - val_accuracy: 0.8378\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 1.2039 - accuracy: 0.8524 - val_loss: 1.5020 - val_accuracy: 0.8393\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 1.1727 - accuracy: 0.8545 - val_loss: 1.5350 - val_accuracy: 0.8402\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 1.1492 - accuracy: 0.8556 - val_loss: 1.5145 - val_accuracy: 0.8376\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 1.1295 - accuracy: 0.8535 - val_loss: 1.5644 - val_accuracy: 0.8405\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 1.0942 - accuracy: 0.8552 - val_loss: 1.4728 - val_accuracy: 0.8420\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 30s 11s/step - loss: 1.0365 - accuracy: 0.8607 - val_loss: 1.5367 - val_accuracy: 0.8416\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.9925 - accuracy: 0.8610 - val_loss: 1.5156 - val_accuracy: 0.8432\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.9458 - accuracy: 0.8631 - val_loss: 1.5081 - val_accuracy: 0.8444\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.8931 - accuracy: 0.8661 - val_loss: 1.5531 - val_accuracy: 0.8451\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.8548 - accuracy: 0.8684 - val_loss: 1.5397 - val_accuracy: 0.8463\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.8033 - accuracy: 0.8733 - val_loss: 1.5702 - val_accuracy: 0.8465\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 31s 12s/step - loss: 0.7550 - accuracy: 0.8777 - val_loss: 1.5817 - val_accuracy: 0.8477\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 33s 12s/step - loss: 0.7170 - accuracy: 0.8827 - val_loss: 1.5878 - val_accuracy: 0.8482\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 38s 12s/step - loss: 0.6709 - accuracy: 0.8884 - val_loss: 1.6209 - val_accuracy: 0.8479\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 33s 12s/step - loss: 0.6268 - accuracy: 0.8924 - val_loss: 1.6219 - val_accuracy: 0.8493\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 32s 12s/step - loss: 0.5902 - accuracy: 0.8987 - val_loss: 1.6349 - val_accuracy: 0.8499\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 31s 12s/step - loss: 0.5451 - accuracy: 0.9045 - val_loss: 1.6498 - val_accuracy: 0.8503\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 33s 12s/step - loss: 0.5083 - accuracy: 0.9088 - val_loss: 1.6593 - val_accuracy: 0.8503\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.4627 - accuracy: 0.9162 - val_loss: 1.6686 - val_accuracy: 0.8518\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.4324 - accuracy: 0.9203 - val_loss: 1.6758 - val_accuracy: 0.8502\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.3955 - accuracy: 0.9272 - val_loss: 1.6849 - val_accuracy: 0.8503\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.3738 - accuracy: 0.9289 - val_loss: 1.6902 - val_accuracy: 0.8509\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 34s 12s/step - loss: 0.3405 - accuracy: 0.9355 - val_loss: 1.6967 - val_accuracy: 0.8506\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.3166 - accuracy: 0.9376 - val_loss: 1.7020 - val_accuracy: 0.8512\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.2933 - accuracy: 0.9407 - val_loss: 1.7082 - val_accuracy: 0.8509\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.2697 - accuracy: 0.9462 - val_loss: 1.7141 - val_accuracy: 0.8524\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 32s 12s/step - loss: 0.2543 - accuracy: 0.9482 - val_loss: 1.7196 - val_accuracy: 0.8517\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.2357 - accuracy: 0.9506 - val_loss: 1.7215 - val_accuracy: 0.8523\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.2186 - accuracy: 0.9552 - val_loss: 1.7294 - val_accuracy: 0.8519\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 32s 12s/step - loss: 0.2056 - accuracy: 0.9565 - val_loss: 1.7357 - val_accuracy: 0.8516\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 34s 12s/step - loss: 0.1925 - accuracy: 0.9594 - val_loss: 1.7406 - val_accuracy: 0.8520\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 32s 10s/step - loss: 0.1803 - accuracy: 0.9613 - val_loss: 1.7435 - val_accuracy: 0.8521\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.1661 - accuracy: 0.9642 - val_loss: 1.7513 - val_accuracy: 0.8524\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 32s 11s/step - loss: 0.1586 - accuracy: 0.9649 - val_loss: 1.7534 - val_accuracy: 0.8521\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 34s 11s/step - loss: 0.1494 - accuracy: 0.9680 - val_loss: 1.7595 - val_accuracy: 0.8523\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.1370 - accuracy: 0.9700 - val_loss: 1.7606 - val_accuracy: 0.8523\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.1335 - accuracy: 0.9712 - val_loss: 1.7658 - val_accuracy: 0.8527\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.1228 - accuracy: 0.9725 - val_loss: 1.7678 - val_accuracy: 0.8522\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 32s 10s/step - loss: 0.1185 - accuracy: 0.9741 - val_loss: 1.7702 - val_accuracy: 0.8515\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.1123 - accuracy: 0.9762 - val_loss: 1.7769 - val_accuracy: 0.8519\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.1036 - accuracy: 0.9774 - val_loss: 1.7830 - val_accuracy: 0.8521\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.1007 - accuracy: 0.9787 - val_loss: 1.7879 - val_accuracy: 0.8528\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.0984 - accuracy: 0.9791 - val_loss: 1.7894 - val_accuracy: 0.8529\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 32s 11s/step - loss: 0.0935 - accuracy: 0.9800 - val_loss: 1.7897 - val_accuracy: 0.8528\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 33s 12s/step - loss: 0.0877 - accuracy: 0.9812 - val_loss: 1.7950 - val_accuracy: 0.8533\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 32s 11s/step - loss: 0.0850 - accuracy: 0.9811 - val_loss: 1.7963 - val_accuracy: 0.8524\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 33s 12s/step - loss: 0.0792 - accuracy: 0.9828 - val_loss: 1.8013 - val_accuracy: 0.8530\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.0776 - accuracy: 0.9830 - val_loss: 1.8017 - val_accuracy: 0.8529\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.0708 - accuracy: 0.9846 - val_loss: 1.8062 - val_accuracy: 0.8530\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.0720 - accuracy: 0.9839 - val_loss: 1.8097 - val_accuracy: 0.8534\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 32s 10s/step - loss: 0.0681 - accuracy: 0.9853 - val_loss: 1.8130 - val_accuracy: 0.8527\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.0641 - accuracy: 0.9865 - val_loss: 1.8168 - val_accuracy: 0.8528\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 32s 12s/step - loss: 0.0619 - accuracy: 0.9859 - val_loss: 1.8178 - val_accuracy: 0.8521\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 31s 12s/step - loss: 0.0599 - accuracy: 0.9869 - val_loss: 1.8189 - val_accuracy: 0.8530\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.0605 - accuracy: 0.9868 - val_loss: 1.8213 - val_accuracy: 0.8523\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.0580 - accuracy: 0.9878 - val_loss: 1.8267 - val_accuracy: 0.8524\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0579 - accuracy: 0.9872 - val_loss: 1.8273 - val_accuracy: 0.8526\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 30s 11s/step - loss: 0.0566 - accuracy: 0.9880 - val_loss: 1.8253 - val_accuracy: 0.8524\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0543 - accuracy: 0.9885 - val_loss: 1.8274 - val_accuracy: 0.8529\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.0513 - accuracy: 0.9886 - val_loss: 1.8293 - val_accuracy: 0.8522\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 34s 11s/step - loss: 0.0506 - accuracy: 0.9894 - val_loss: 1.8394 - val_accuracy: 0.8526\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 34s 11s/step - loss: 0.0514 - accuracy: 0.9883 - val_loss: 1.8390 - val_accuracy: 0.8510\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 34s 11s/step - loss: 0.0475 - accuracy: 0.9891 - val_loss: 1.8414 - val_accuracy: 0.8524\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.0502 - accuracy: 0.9894 - val_loss: 1.8404 - val_accuracy: 0.8522\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.0477 - accuracy: 0.9899 - val_loss: 1.8541 - val_accuracy: 0.8524\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.0560 - accuracy: 0.9876 - val_loss: 1.8495 - val_accuracy: 0.8491\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.0536 - accuracy: 0.9871 - val_loss: 1.8468 - val_accuracy: 0.8522\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.0535 - accuracy: 0.9876 - val_loss: 1.9053 - val_accuracy: 0.8519\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.1611 - accuracy: 0.9681 - val_loss: 1.8428 - val_accuracy: 0.8524\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.0921 - accuracy: 0.9806 - val_loss: 1.8852 - val_accuracy: 0.8521\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 32s 11s/step - loss: 0.1348 - accuracy: 0.9724 - val_loss: 1.8378 - val_accuracy: 0.8510\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 32s 10s/step - loss: 0.0951 - accuracy: 0.9791 - val_loss: 1.8350 - val_accuracy: 0.8457\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0960 - accuracy: 0.9787 - val_loss: 1.8271 - val_accuracy: 0.8535\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0766 - accuracy: 0.9841 - val_loss: 1.8423 - val_accuracy: 0.8538\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0829 - accuracy: 0.9818 - val_loss: 1.8261 - val_accuracy: 0.8533\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 29s 9s/step - loss: 0.0688 - accuracy: 0.9849 - val_loss: 1.8130 - val_accuracy: 0.8517\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0655 - accuracy: 0.9861 - val_loss: 1.8091 - val_accuracy: 0.8514\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0630 - accuracy: 0.9861 - val_loss: 1.8106 - val_accuracy: 0.8529\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0573 - accuracy: 0.9880 - val_loss: 1.8140 - val_accuracy: 0.8535\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0561 - accuracy: 0.9883 - val_loss: 1.8168 - val_accuracy: 0.8541\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 29s 9s/step - loss: 0.0496 - accuracy: 0.9891 - val_loss: 1.8213 - val_accuracy: 0.8539\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 28s 10s/step - loss: 0.0523 - accuracy: 0.9890 - val_loss: 1.8236 - val_accuracy: 0.8541\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 29s 9s/step - loss: 0.0490 - accuracy: 0.9894 - val_loss: 1.8254 - val_accuracy: 0.8555\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0471 - accuracy: 0.9893 - val_loss: 1.8301 - val_accuracy: 0.8558\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0453 - accuracy: 0.9905 - val_loss: 1.8328 - val_accuracy: 0.8555\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 29s 9s/step - loss: 0.0426 - accuracy: 0.9907 - val_loss: 1.8346 - val_accuracy: 0.8555\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0425 - accuracy: 0.9911 - val_loss: 1.8365 - val_accuracy: 0.8550\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 29s 9s/step - loss: 0.0425 - accuracy: 0.9907 - val_loss: 1.8375 - val_accuracy: 0.8547\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0386 - accuracy: 0.9910 - val_loss: 1.8391 - val_accuracy: 0.8550\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0407 - accuracy: 0.9911 - val_loss: 1.8417 - val_accuracy: 0.8553\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0389 - accuracy: 0.9914 - val_loss: 1.8442 - val_accuracy: 0.8552\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0360 - accuracy: 0.9912 - val_loss: 1.8465 - val_accuracy: 0.8543\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0374 - accuracy: 0.9917 - val_loss: 1.8479 - val_accuracy: 0.8537\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 29s 9s/step - loss: 0.0383 - accuracy: 0.9912 - val_loss: 1.8490 - val_accuracy: 0.8536\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0378 - accuracy: 0.9916 - val_loss: 1.8507 - val_accuracy: 0.8537\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 29s 9s/step - loss: 0.0374 - accuracy: 0.9915 - val_loss: 1.8528 - val_accuracy: 0.8541\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0346 - accuracy: 0.9916 - val_loss: 1.8543 - val_accuracy: 0.8546\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0379 - accuracy: 0.9915 - val_loss: 1.8550 - val_accuracy: 0.8541\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0342 - accuracy: 0.9920 - val_loss: 1.8549 - val_accuracy: 0.8539\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 28s 10s/step - loss: 0.0355 - accuracy: 0.9921 - val_loss: 1.8544 - val_accuracy: 0.8535\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 29s 9s/step - loss: 0.0332 - accuracy: 0.9919 - val_loss: 1.8542 - val_accuracy: 0.8540\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0348 - accuracy: 0.9921 - val_loss: 1.8550 - val_accuracy: 0.8539\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0328 - accuracy: 0.9919 - val_loss: 1.8562 - val_accuracy: 0.8540\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0342 - accuracy: 0.9923 - val_loss: 1.8568 - val_accuracy: 0.8538\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0342 - accuracy: 0.9924 - val_loss: 1.8561 - val_accuracy: 0.8539\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0337 - accuracy: 0.9926 - val_loss: 1.8549 - val_accuracy: 0.8538\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0338 - accuracy: 0.9922 - val_loss: 1.8552 - val_accuracy: 0.8536\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 29s 9s/step - loss: 0.0332 - accuracy: 0.9927 - val_loss: 1.8568 - val_accuracy: 0.8540\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 29s 9s/step - loss: 0.0321 - accuracy: 0.9922 - val_loss: 1.8591 - val_accuracy: 0.8544\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0343 - accuracy: 0.9919 - val_loss: 1.8617 - val_accuracy: 0.8544\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 28s 9s/step - loss: 0.0322 - accuracy: 0.9923 - val_loss: 1.8635 - val_accuracy: 0.8547\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 30s 11s/step - loss: 0.0343 - accuracy: 0.9921 - val_loss: 1.8641 - val_accuracy: 0.8550\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 30s 11s/step - loss: 0.0335 - accuracy: 0.9924 - val_loss: 1.8636 - val_accuracy: 0.8551\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 29s 9s/step - loss: 0.0339 - accuracy: 0.9917 - val_loss: 1.8625 - val_accuracy: 0.8550\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 28s 10s/step - loss: 0.0328 - accuracy: 0.9922 - val_loss: 1.8620 - val_accuracy: 0.8550\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0336 - accuracy: 0.9922 - val_loss: 1.8626 - val_accuracy: 0.8545\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 30s 11s/step - loss: 0.0335 - accuracy: 0.9923 - val_loss: 1.8632 - val_accuracy: 0.8544\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.0328 - accuracy: 0.9924 - val_loss: 1.8633 - val_accuracy: 0.8544\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.0307 - accuracy: 0.9929 - val_loss: 1.8644 - val_accuracy: 0.8546\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 32s 11s/step - loss: 0.0304 - accuracy: 0.9928 - val_loss: 1.8662 - val_accuracy: 0.8544\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.0295 - accuracy: 0.9927 - val_loss: 1.8674 - val_accuracy: 0.8542\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.0331 - accuracy: 0.9924 - val_loss: 1.8679 - val_accuracy: 0.8538\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.0328 - accuracy: 0.9923 - val_loss: 1.8674 - val_accuracy: 0.8542\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.0331 - accuracy: 0.9924 - val_loss: 1.8661 - val_accuracy: 0.8541\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.0318 - accuracy: 0.9922 - val_loss: 1.8652 - val_accuracy: 0.8538\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.0323 - accuracy: 0.9921 - val_loss: 1.8655 - val_accuracy: 0.8534\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 30s 11s/step - loss: 0.0301 - accuracy: 0.9923 - val_loss: 1.8668 - val_accuracy: 0.8542\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 30s 11s/step - loss: 0.0312 - accuracy: 0.9924 - val_loss: 1.8681 - val_accuracy: 0.8541\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 30s 11s/step - loss: 0.0316 - accuracy: 0.9927 - val_loss: 1.8695 - val_accuracy: 0.8538\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.0306 - accuracy: 0.9928 - val_loss: 1.8697 - val_accuracy: 0.8541\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.0320 - accuracy: 0.9928 - val_loss: 1.8691 - val_accuracy: 0.8540\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0311 - accuracy: 0.9927 - val_loss: 1.8678 - val_accuracy: 0.8541\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0310 - accuracy: 0.9927 - val_loss: 1.8667 - val_accuracy: 0.8539\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0309 - accuracy: 0.9927 - val_loss: 1.8665 - val_accuracy: 0.8538\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0312 - accuracy: 0.9925 - val_loss: 1.8666 - val_accuracy: 0.8538\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0310 - accuracy: 0.9925 - val_loss: 1.8668 - val_accuracy: 0.8539\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0319 - accuracy: 0.9925 - val_loss: 1.8673 - val_accuracy: 0.8538\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0305 - accuracy: 0.9926 - val_loss: 1.8682 - val_accuracy: 0.8540\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0286 - accuracy: 0.9925 - val_loss: 1.8695 - val_accuracy: 0.8539\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0315 - accuracy: 0.9925 - val_loss: 1.8707 - val_accuracy: 0.8542\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.0315 - accuracy: 0.9923 - val_loss: 1.8721 - val_accuracy: 0.8543\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0289 - accuracy: 0.9925 - val_loss: 1.8734 - val_accuracy: 0.8543\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 28s 10s/step - loss: 0.0291 - accuracy: 0.9926 - val_loss: 1.8744 - val_accuracy: 0.8545\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0319 - accuracy: 0.9924 - val_loss: 1.8760 - val_accuracy: 0.8541\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0318 - accuracy: 0.9925 - val_loss: 1.8779 - val_accuracy: 0.8534\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0312 - accuracy: 0.9923 - val_loss: 1.8793 - val_accuracy: 0.8533\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0303 - accuracy: 0.9928 - val_loss: 1.8800 - val_accuracy: 0.8534\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0282 - accuracy: 0.9924 - val_loss: 1.8795 - val_accuracy: 0.8534\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0314 - accuracy: 0.9925 - val_loss: 1.8777 - val_accuracy: 0.8539\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0310 - accuracy: 0.9926 - val_loss: 1.8758 - val_accuracy: 0.8535\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0307 - accuracy: 0.9928 - val_loss: 1.8745 - val_accuracy: 0.8536\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0296 - accuracy: 0.9927 - val_loss: 1.8744 - val_accuracy: 0.8529\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0286 - accuracy: 0.9926 - val_loss: 1.8754 - val_accuracy: 0.8530\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0297 - accuracy: 0.9926 - val_loss: 1.8769 - val_accuracy: 0.8531\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 32s 11s/step - loss: 0.0286 - accuracy: 0.9923 - val_loss: 1.8777 - val_accuracy: 0.8531\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.0297 - accuracy: 0.9923 - val_loss: 1.8786 - val_accuracy: 0.8534\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0287 - accuracy: 0.9926 - val_loss: 1.8793 - val_accuracy: 0.8533\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 32s 11s/step - loss: 0.0288 - accuracy: 0.9925 - val_loss: 1.8794 - val_accuracy: 0.8533\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0301 - accuracy: 0.9927 - val_loss: 1.8792 - val_accuracy: 0.8534\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0309 - accuracy: 0.9926 - val_loss: 1.8787 - val_accuracy: 0.8534\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 32s 10s/step - loss: 0.0299 - accuracy: 0.9929 - val_loss: 1.8785 - val_accuracy: 0.8534\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 31s 12s/step - loss: 0.0304 - accuracy: 0.9926 - val_loss: 1.8786 - val_accuracy: 0.8533\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.0303 - accuracy: 0.9927 - val_loss: 1.8781 - val_accuracy: 0.8528\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 30s 11s/step - loss: 0.0286 - accuracy: 0.9927 - val_loss: 1.8778 - val_accuracy: 0.8529\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 32s 10s/step - loss: 0.0295 - accuracy: 0.9930 - val_loss: 1.8779 - val_accuracy: 0.8529\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 30s 11s/step - loss: 0.0315 - accuracy: 0.9925 - val_loss: 1.8779 - val_accuracy: 0.8532\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.0279 - accuracy: 0.9929 - val_loss: 1.8783 - val_accuracy: 0.8535\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.0284 - accuracy: 0.9925 - val_loss: 1.8790 - val_accuracy: 0.8537\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 32s 10s/step - loss: 0.0304 - accuracy: 0.9927 - val_loss: 1.8799 - val_accuracy: 0.8541\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 32s 10s/step - loss: 0.0294 - accuracy: 0.9925 - val_loss: 1.8811 - val_accuracy: 0.8541\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.0302 - accuracy: 0.9929 - val_loss: 1.8820 - val_accuracy: 0.8536\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 31s 11s/step - loss: 0.0302 - accuracy: 0.9922 - val_loss: 1.8824 - val_accuracy: 0.8536\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.0303 - accuracy: 0.9923 - val_loss: 1.8831 - val_accuracy: 0.8533\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.0312 - accuracy: 0.9927 - val_loss: 1.8843 - val_accuracy: 0.8531\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.0299 - accuracy: 0.9927 - val_loss: 1.8855 - val_accuracy: 0.8530\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.0309 - accuracy: 0.9924 - val_loss: 1.8862 - val_accuracy: 0.8532\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0286 - accuracy: 0.9927 - val_loss: 1.8867 - val_accuracy: 0.8538\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.0287 - accuracy: 0.9927 - val_loss: 1.8869 - val_accuracy: 0.8535\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0278 - accuracy: 0.9930 - val_loss: 1.8871 - val_accuracy: 0.8536\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0310 - accuracy: 0.9922 - val_loss: 1.8869 - val_accuracy: 0.8536\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 31s 10s/step - loss: 0.0300 - accuracy: 0.9925 - val_loss: 1.8866 - val_accuracy: 0.8538\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 32s 10s/step - loss: 0.0294 - accuracy: 0.9927 - val_loss: 1.8857 - val_accuracy: 0.8536\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 32s 10s/step - loss: 0.0303 - accuracy: 0.9926 - val_loss: 1.8852 - val_accuracy: 0.8534\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.0285 - accuracy: 0.9925 - val_loss: 1.8844 - val_accuracy: 0.8531\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 32s 10s/step - loss: 0.0309 - accuracy: 0.9926 - val_loss: 1.8831 - val_accuracy: 0.8534\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.0284 - accuracy: 0.9926 - val_loss: 1.8829 - val_accuracy: 0.8538\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 30s 10s/step - loss: 0.0285 - accuracy: 0.9928 - val_loss: 1.8833 - val_accuracy: 0.8535\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 33s 11s/step - loss: 0.0301 - accuracy: 0.9928 - val_loss: 1.8840 - val_accuracy: 0.8535\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 30s 11s/step - loss: 0.0278 - accuracy: 0.9925 - val_loss: 1.8846 - val_accuracy: 0.8539\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 30s 11s/step - loss: 0.0300 - accuracy: 0.9925 - val_loss: 1.8857 - val_accuracy: 0.8537\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 32s 10s/step - loss: 0.0302 - accuracy: 0.9923 - val_loss: 1.8872 - val_accuracy: 0.8535\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 33s 10s/step - loss: 0.0298 - accuracy: 0.9927 - val_loss: 1.8882 - val_accuracy: 0.8532\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 33s 10s/step - loss: 0.0285 - accuracy: 0.9926 - val_loss: 1.8886 - val_accuracy: 0.8534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvMkJuBrYP0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ce6338-9559-420d-b8bd-ced05a3b0dd5"
      },
      "source": [
        "scores = rnn_model.evaluate(tmp_x, preproc_french_sentences, verbose=0)\n",
        "# Displays the accuracy of correct sentiment prediction over test data\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 96.51%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTx4ARR9NGgl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "2dafefc9-bd60-402a-f9fe-945cf5dc1fc3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_44LGpK-xx4"
      },
      "source": [
        "def final_predictions(text):\n",
        "  y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
        "  y_id_to_word[0] = '<PAD>'\n",
        "\n",
        "  sentence = [english_tokenizer.word_index[word] for word in text.split()]\n",
        "  sentence = pad_sequences([sentence], maxlen=preproc_french_sentences.shape[-2], padding='post')\n",
        "  \n",
        "  print(sentence.shape)\n",
        "  print(logits_to_text(simple_rnn_model.predict(sentence[:1])[0], french_tokenizer))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm-DAR9H-4xD"
      },
      "source": [
        "rnn_model.save('model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onnTqdkf-_ry"
      },
      "source": [
        "! mkdir \"pickles\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhDpg-NB_AXV"
      },
      "source": [
        "with open('pickles/french_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(french_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('pickles/english_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(english_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('pickles/preproc_french_sentences.pickle', 'wb') as handle:\n",
        "    pickle.dump(preproc_french_sentences, handle, protocol=pickle.HIGHEST_PROTOCOL) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}